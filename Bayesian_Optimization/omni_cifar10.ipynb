{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb49f83-8678-4bcb-ad08-4e39e258e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a MobileNetV3 Model as defined in:\n",
    "Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam. (2019).\n",
    "Searching for MobileNetV3\n",
    "arXiv preprint arXiv:1905.02244.\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from omni import *\n",
    "from mixup import *\n",
    "\n",
    "__all__ = ['mobilenetv3_large', 'mobilenetv3_small']\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "class h_sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = nn.ReLU6(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "\n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(channel, _make_divisible(channel // reduction, 8)),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(_make_divisible(channel // reduction, 8), channel),\n",
    "                h_sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8892c0-806b-4134-947e-609c4a2092b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_3x3(inp, oup, stride, batch_norm = True):\n",
    "    layers = [\n",
    "        nn.Conv2d(inp, oup, kernel_size=3, stride=1,\n",
    "                  padding=1, bias=False),\n",
    "        h_swish()\n",
    "    ]#\n",
    "    if batch_norm:\n",
    "        layers.insert(1, nn.BatchNorm2d(oup))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def conv_1x1(inp, oup, batch_norm = True):\n",
    "    layers = [\n",
    "        nn.Conv2d(inp, oup, kernel_size=1, stride=1,\n",
    "                  padding=0, bias=False),\n",
    "        h_swish()\n",
    "    ]\n",
    "    if batch_norm:\n",
    "        layers.insert(1, nn.BatchNorm2d(oup))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8affe328-a0e9-4cb8-93b9-c105c3567bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def od_conv_1x1(inp, oup, stride = 1, kernel_num = 4,\n",
    "                temperature = 60,\n",
    "                batch_norm = True):\n",
    "    return nn.Sequential(\n",
    "        ODConvBN(inp, oup, kernel_size = 1, stride = stride,\n",
    "            kernel_num = kernel_num, temperature = temperature) \\\n",
    "                if batch_norm == True else \\\n",
    "        ODConv2d(inp, oup, kernel_size = 1, stride = stride,\n",
    "            kernel_num = kernel_num, temperature = temperature),\n",
    "\n",
    "        h_swish()\n",
    "    )\n",
    "\n",
    "def od_conv_3x3(inp, oup, stride = 1,\n",
    "                kernel_num = 4, temperature = 60,\n",
    "                batch_norm = True):\n",
    "    return nn.Sequential(\n",
    "         ODConvBN(inp, oup, kernel_size = 3, stride = stride,\n",
    "            kernel_num = kernel_num, temperature = temperature) \\\n",
    "                if batch_norm == True else \\\n",
    "        ODConv2d(inp, oup, kernel_size = 3, stride = stride,\n",
    "            kernel_num = kernel_num, temperature = temperature),\n",
    "\n",
    "        h_swish()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6936ef6d-ce0d-4865-9ba8-fc465768c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedResidualOD(nn.Module):\n",
    "    def __init__(self, inp, hidden_dim, oup, kernel_size,\n",
    "                 stride, use_se, use_hs,\n",
    "                 kernel_num_1 = 4,\n",
    "                 kernel_num_2 = 4, temperature = 60.0):\n",
    "        super(InvertedResidualOD, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        self.identity = stride == 1 and inp == oup\n",
    "        print(\"Using OmniDimensional\")\n",
    "        if inp == hidden_dim:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                ODConvBN(hidden_dim, hidden_dim, kernel_size, stride,\n",
    "                         groups=hidden_dim, kernel_num = kernel_num_1,\n",
    "                         temperature = temperature),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                # pw-linear\n",
    "                ODConv2d(hidden_dim, oup, 1, 1, kernel_num = kernel_num_2,\n",
    "                         temperature = temperature),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                ODConvBN(inp, hidden_dim, kernel_size = 1, stride = 1,\n",
    "                         kernel_num = kernel_num_2, temperature = temperature),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # dw\n",
    "                ODConvBN(hidden_dim, hidden_dim, kernel_size,\n",
    "                         stride, groups=hidden_dim, kernel_num = kernel_num_1,\n",
    "                         temperature = temperature),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # pw-linear\n",
    "                ODConv2d(hidden_dim, oup, 1, 1, kernel_num = kernel_num_2,\n",
    "                         temperature = temperature),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b3b9f6e-d94c-48cb-9be7-33ee2ad4e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se, use_hs):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        self.identity = stride == 1 and inp == oup\n",
    "\n",
    "        print(\"Using Normal\")\n",
    "        if inp == hidden_dim:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "    \n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a80af455-f7fb-4611-a3f9-77175748195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV3(nn.Module):\n",
    "    def __init__(self, cfgs, mode, kernel_num_1, kernel_num_2,\n",
    "                 temperature, od_bottleneck = 0, \n",
    "                 od_outside = 0, num_classes = 10, width_mult=1.,\n",
    "                 use_od = False, drop_rate = 0.2):\n",
    "        super(MobileNetV3, self).__init__()\n",
    "        # setting of inverted residual blocks\n",
    "        self.cfgs = cfgs\n",
    "        self.use_od = use_od\n",
    "        assert mode in ['large', 'small']\n",
    "\n",
    "        num_od = int(od_outside)\n",
    "        od_bottleneck = int(od_bottleneck)\n",
    "        \n",
    "        self.num_od = num_od\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(16 * width_mult, 8)\n",
    "\n",
    "        layers = []\n",
    "        if num_od > 0:\n",
    "            print(\"Using OD\")\n",
    "            layers.append(od_conv_3x3(3, input_channel, stride = 2,\n",
    "                                      kernel_num = kernel_num_1,\n",
    "                                      temperature = temperature))\n",
    "        else:\n",
    "            print(\"Using Normal\")\n",
    "            layers.append(conv_3x3(3, input_channel, stride = 2))\n",
    "        \n",
    "        # building inverted residual blocks\n",
    "        \n",
    "\n",
    "        i = 0\n",
    "        for k, t, c, use_se, use_hs, s in self.cfgs:\n",
    "            output_channel = _make_divisible(c * width_mult, 8)\n",
    "            exp_size = _make_divisible(input_channel * t, 8)\n",
    "            block = InvertedResidual if (use_od == False or i >= od_bottleneck) \\\n",
    "                        else InvertedResidualOD\n",
    "            \n",
    "            if use_od == False or i >= od_bottleneck:\n",
    "                layers.append(block(input_channel, exp_size, output_channel,\n",
    "                                    k, s, use_se, use_hs))\n",
    "            else:\n",
    "                layers.append(block(input_channel, exp_size, output_channel,\n",
    "                                    k, s, use_se, use_hs, kernel_num_1 = kernel_num_1,\n",
    "                                    kernel_num_2 = kernel_num_2,\n",
    "                                    temperature = temperature))\n",
    "                i += 1\n",
    "            \n",
    "            input_channel = output_channel\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "        print(\"Using OD\")\n",
    "        self.conv = od_conv_1x1(input_channel, exp_size,\n",
    "                                    kernel_num = kernel_num_2,\n",
    "                                    temperature = temperature)\n",
    "        \n",
    "        # building last several layers\n",
    "        # if num_od >= 2:\n",
    "        #     # print(\"Using OD\")\n",
    "        #     self.conv = od_conv_1x1(input_channel, exp_size,\n",
    "        #                             kernel_num = kernel_num,\n",
    "        #                             temperature = temperature)\n",
    "        # else:\n",
    "        #     # print(\"Using Normal\")\n",
    "        #     self.conv = conv_1x1(input_channel, exp_size)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        output_channel = {'large': 1280, 'small': 1024}\n",
    "\n",
    "        output_channel = _make_divisible(output_channel[mode] * width_mult, 8) if width_mult > 1.0 else output_channel[mode]\n",
    "\n",
    "        omni_layers = []\n",
    "        not_omni_layers = []\n",
    "        temp_od = num_od - 2\n",
    "\n",
    "        self.list_layers = []\n",
    "        for i in range(2):\n",
    "            if temp_od > 0:\n",
    "                self.list_layers.append(\"OD\")\n",
    "            else:\n",
    "                self.list_layers.append(\"Normal\")\n",
    "\n",
    "            temp_od -= 1\n",
    "\n",
    "\n",
    "        input_channel = exp_size\n",
    "        for layer in self.list_layers:\n",
    "            if layer == \"OD\":\n",
    "                print(\"Using OD\")\n",
    "                omni_layers.append(od_conv_1x1(input_channel, output_channel,\n",
    "                                          kernel_num = kernel_num,\n",
    "                                          temperature = temperature))\n",
    "            else:\n",
    "                print(\"Using Normal\")\n",
    "                not_omni_layers.append(nn.Linear(input_channel, output_channel))\n",
    "\n",
    "            input_channel = output_channel\n",
    "            output_channel = num_classes\n",
    "\n",
    "        self.omni_layers = nn.Sequential(*omni_layers)\n",
    "        self.normal_layers = nn.Sequential(*not_omni_layers)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.omni_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.normal_layers(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def net_update_temperature(self, temperature):\n",
    "        for modules in self.modules():\n",
    "            if hasattr(modules, \"update_temperature\"):\n",
    "                modules.update_temperature(temperature)\n",
    "\n",
    "    def display_temperature(self):\n",
    "        for modules in self.modules():\n",
    "            if hasattr(modules, \"get_temperature\"):\n",
    "                return modules.get_temperature()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da42216-a946-4e17-a99d-786ae43c4943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilenetv3_large(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV3-Large model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # k, t,   c,  SE, HS, s\n",
    "        [3,   1,  16, 0, 0, 1],\n",
    "        [3,   4,  24, 0, 0, 2],\n",
    "        [3,   3,  24, 0, 0, 1],\n",
    "        [5,   3,  40, 1, 0, 2],\n",
    "        [5,   3,  40, 1, 0, 1],\n",
    "        [5,   3,  40, 1, 0, 1],\n",
    "        [3,   6,  80, 0, 1, 2],\n",
    "        [3, 2.5,  80, 0, 1, 1],\n",
    "        [3, 2.3,  80, 0, 1, 1],\n",
    "        [3, 2.3,  80, 0, 1, 1],\n",
    "        [3,   6, 112, 1, 1, 1],\n",
    "        [3,   6, 112, 1, 1, 1],\n",
    "        [5,   6, 160, 1, 1, 2],\n",
    "        [5,   6, 160, 1, 1, 1],\n",
    "        [5,   6, 160, 1, 1, 1]\n",
    "    ]\n",
    "    return MobileNetV3(cfgs, mode='large', **kwargs)\n",
    "\n",
    "\n",
    "def mobilenetv3_small(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV3-Small model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # k,   t,  c, SE, HS, s\n",
    "        [3,    1,  16, 1, 0, 2],\n",
    "        [3,  4.5,  24, 0, 0, 2],\n",
    "        [3, 3.67,  24, 0, 0, 1],\n",
    "        [5,    4,  40, 1, 1, 2],\n",
    "        [5,    6,  40, 1, 1, 1],\n",
    "        [5,    6,  40, 1, 1, 1],\n",
    "        [5,    3,  48, 1, 1, 1],\n",
    "        [5,    3,  48, 1, 1, 1],\n",
    "        [5,    6,  96, 1, 1, 2],\n",
    "        [5,    6,  96, 1, 1, 1],\n",
    "        [5,    6,  96, 1, 1, 1],\n",
    "    ]\n",
    "\n",
    "    return MobileNetV3(cfgs, mode='small', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7ab8ba8-7b86-4b56-9291-cc655cdbba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af0ebbf6-4ede-4a88-bc21-ba54c4a33f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45b6d5f5-93dc-4131-823b-69759f8def1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_dir, download = True):\n",
    "\n",
    "  transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "  ])\n",
    "\n",
    "  train_data = datasets.CIFAR10(\n",
    "      root = data_dir, train = True,\n",
    "      download = download, transform = transform\n",
    "  )\n",
    "\n",
    "  test_data = datasets.CIFAR10(\n",
    "      root = data_dir, train = False,\n",
    "      download = download, transform = transform\n",
    "  )\n",
    "\n",
    "  return (train_data, test_data)\n",
    "\n",
    "train_data, test_data = load_data('./data/cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8931d1a1-7b4c-4f75-90ca-339106afa3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "args = Namespace(\n",
    "    data_dir = './data/cifar10',\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    batch_size = 128,\n",
    "    num_workers = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c627cb09-0d6b-4b1d-a4ea-341f83d81dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = args.batch_size,\n",
    "                          shuffle = True, num_workers = args.num_workers)\n",
    "test_loader = DataLoader(test_data, batch_size = args.batch_size,\n",
    "                         shuffle = True, num_workers = args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04853534-3ee6-4215-8d3d-c60b2379a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def check_logging_directory(path):\n",
    "  parent_directory = os.path.dirname(path)\n",
    "  if not os.path.exists(parent_directory):\n",
    "    os.makedirs(parent_directory)\n",
    "    print(\"Create new directory\")\n",
    "\n",
    "logging_path = './logging/bayesian_omni_cifar10.log'\n",
    "check_logging_directory(logging_path)\n",
    "\n",
    "logging.basicConfig(filename=logging_path, level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b1138d6-18e5-4a65-a1a4-2b5378ebd9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Normal\n",
      "Using OmniDimensional\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using OD\n",
      "Using Normal\n",
      "Using Normal\n",
      "The number of parameters: 1598441\n"
     ]
    }
   ],
   "source": [
    "kernel_num_1 = 2\n",
    "kernel_num_2 = 2\n",
    "od_bottleneck = 1\n",
    "temperature = 31\n",
    "dropout = 0.07982333339968264\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "mb_v3 = mobilenetv3_small(num_classes = 10, od_bottleneck = od_bottleneck,\n",
    "                             od_outside = 0, \n",
    "                          kernel_num_1 = kernel_num_1,\n",
    "                            kernel_num_2 = kernel_num_2,\n",
    "                            temperature = temperature,\n",
    "                          use_od = True, drop_rate = dropout).to(args.device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(args.device)\n",
    "optimizer = torch.optim.SGD(mb_v3.parameters(), lr=learning_rate,\n",
    "                            weight_decay = 0.00004, momentum = 0.9)\n",
    "\n",
    "print(f\"The number of parameters: {count_parameters(mb_v3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9faab4dc-b358-4b3f-8ed4-507ed66ed653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, total_epochs,\n",
    "                         iteration, iter_per_epoch, initial_lr = 0.05):\n",
    "    current_iter = iteration + epoch * iter_per_epoch\n",
    "    max_iter = total_epochs * iter_per_epoch\n",
    "\n",
    "    lr = initial_lr * (1 + np.cos(np.pi * current_iter / max_iter)) / 2\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def get_temperature(iteration, epoch, iter_per_epoch,\n",
    "                        temp_epoch = 10, temp_init = 30.0):\n",
    "    total_temp_iter = iter_per_epoch * temp_epoch\n",
    "    current_iter = iteration + epoch * iter_per_epoch\n",
    "    # print(current_iter)\n",
    "    temperature = 1.0 + max(0, (temp_init - 1.0) * \\\n",
    "                            ((total_temp_iter - current_iter) / \\\n",
    "                            total_temp_iter))\n",
    "    return temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0214bd-fb91-4e20-8fdf-d3d36bf1c664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d8d0069de34d38a230c5a63928bd07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d916994dbdd4c16bdb70bebce4baf9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc70bbf6a56496a9965bb7045ec6319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training MobileNetV3 - Omni Dimensional Dynamic Convolution ðŸš€\n",
      "========================================\n",
      "\u001b[1;34mEpoch 1/100\u001b[0m\n",
      "Train Loss: 1.70\t|\tTrain Acc: 38.56%\n",
      "Val Loss: 1.41\t|\tVal Acc: 47.94%\n",
      "The current temperature is: 30.4\n",
      "The current learning rate is: 0.0499876640091433\n",
      "========================================\n",
      "\u001b[1;34mEpoch 2/100\u001b[0m\n",
      "Train Loss: 1.30\t|\tTrain Acc: 57.01%\n",
      "Val Loss: 1.02\t|\tVal Acc: 64.07%\n",
      "The current temperature is: 29.223999999999997\n",
      "The current learning rate is: 0.04995066821070679\n",
      "========================================\n",
      "\u001b[1;34mEpoch 3/100\u001b[0m\n",
      "Train Loss: 1.14\t|\tTrain Acc: 63.67%\n",
      "Val Loss: 0.86\t|\tVal Acc: 72.92%\n",
      "The current temperature is: 27.530559999999994\n",
      "The current learning rate is: 0.049889049115076994\n",
      "========================================\n",
      "\u001b[1;34mEpoch 4/100\u001b[0m\n",
      "Train Loss: 1.07\t|\tTrain Acc: 67.17%\n",
      "Val Loss: 0.71\t|\tVal Acc: 77.53%\n",
      "The current temperature is: 25.408115199999994\n",
      "The current learning rate is: 0.04980286753286194\n",
      "========================================\n",
      "\u001b[1;34mEpoch 5/100\u001b[0m\n",
      "Train Loss: 0.97\t|\tTrain Acc: 71.51%\n",
      "Val Loss: 0.70\t|\tVal Acc: 78.42%\n",
      "The current temperature is: 22.967303679999993\n",
      "The current learning rate is: 0.04969220851487845\n",
      "========================================\n",
      "\u001b[1;34mEpoch 6/100\u001b[0m\n",
      "Train Loss: 0.89\t|\tTrain Acc: 74.39%\n",
      "Val Loss: 0.65\t|\tVal Acc: 79.39%\n",
      "The current temperature is: 20.331227238399993\n",
      "The current learning rate is: 0.04955718126821722\n",
      "========================================\n",
      "\u001b[1;34mEpoch 7/100\u001b[0m\n",
      "Train Loss: 0.86\t|\tTrain Acc: 75.41%\n",
      "Val Loss: 0.58\t|\tVal Acc: 81.37%\n",
      "The current temperature is: 17.624855425023995\n",
      "The current learning rate is: 0.049397919048468686\n",
      "========================================\n",
      "\u001b[1;34mEpoch 8/100\u001b[0m\n",
      "Train Loss: 0.83\t|\tTrain Acc: 76.45%\n",
      "Val Loss: 0.56\t|\tVal Acc: 81.84%\n",
      "The current temperature is: 14.964878557020155\n",
      "The current learning rate is: 0.049214579028215785\n",
      "========================================\n",
      "\u001b[1;34mEpoch 9/100\u001b[0m\n",
      "Train Loss: 0.77\t|\tTrain Acc: 78.56%\n",
      "Val Loss: 0.56\t|\tVal Acc: 82.49%\n",
      "The current temperature is: 12.451200416756526\n",
      "The current learning rate is: 0.049007342141923586\n",
      "========================================\n",
      "\u001b[1;34mEpoch 10/100\u001b[0m\n",
      "Train Loss: 0.73\t|\tTrain Acc: 79.84%\n",
      "Val Loss: 0.52\t|\tVal Acc: 83.07%\n",
      "The current temperature is: 10.160960333405221\n",
      "The current learning rate is: 0.048776412907378844\n",
      "========================================\n",
      "\u001b[1;34mEpoch 11/100\u001b[0m\n",
      "Train Loss: 0.70\t|\tTrain Acc: 80.75%\n",
      "Val Loss: 0.47\t|\tVal Acc: 84.76%\n",
      "The current temperature is: 8.145549060056073\n",
      "The current learning rate is: 0.04852201922385564\n",
      "========================================\n",
      "\u001b[1;34mEpoch 12/100\u001b[0m\n",
      "Train Loss: 0.70\t|\tTrain Acc: 80.96%\n",
      "Val Loss: 0.47\t|\tVal Acc: 85.15%\n",
      "The current temperature is: 6.430617285642615\n",
      "The current learning rate is: 0.04824441214720629\n",
      "========================================\n",
      "\u001b[1;34mEpoch 13/100\u001b[0m\n",
      "Train Loss: 0.71\t|\tTrain Acc: 81.39%\n",
      "Val Loss: 0.49\t|\tVal Acc: 86.10%\n",
      "The current temperature is: 5.018656791375535\n",
      "The current learning rate is: 0.04794386564209953\n",
      "========================================\n",
      "\u001b[1;34mEpoch 14/100\u001b[0m\n",
      "Train Loss: 0.66\t|\tTrain Acc: 82.56%\n",
      "Val Loss: 0.44\t|\tVal Acc: 86.78%\n",
      "The current temperature is: 3.893432889790385\n",
      "The current learning rate is: 0.047620676311650484\n",
      "========================================\n",
      "\u001b[1;34mEpoch 15/100\u001b[0m\n",
      "Train Loss: 0.66\t|\tTrain Acc: 82.30%\n",
      "Val Loss: 0.46\t|\tVal Acc: 86.44%\n",
      "The current temperature is: 3.0254030228532693\n",
      "The current learning rate is: 0.047275163104709195\n",
      "========================================\n",
      "\u001b[1;34mEpoch 16/100\u001b[0m\n",
      "Train Loss: 0.64\t|\tTrain Acc: 83.57%\n",
      "Val Loss: 0.44\t|\tVal Acc: 86.71%\n",
      "The current temperature is: 2.377274055540223\n",
      "The current learning rate is: 0.04690766700109659\n",
      "========================================\n",
      "\u001b[1;34mEpoch 17/100\u001b[0m\n",
      "Train Loss: 0.65\t|\tTrain Acc: 82.70%\n",
      "Val Loss: 0.40\t|\tVal Acc: 87.27%\n",
      "The current temperature is: 1.9090008766565472\n",
      "The current learning rate is: 0.046518550675098594\n",
      "========================================\n",
      "\u001b[1;34mEpoch 18/100\u001b[0m\n",
      "Train Loss: 0.57\t|\tTrain Acc: 85.16%\n",
      "Val Loss: 0.41\t|\tVal Acc: 86.85%\n",
      "The current temperature is: 1.5817605610601904\n",
      "The current learning rate is: 0.04610819813755038\n",
      "========================================\n",
      "\u001b[1;34mEpoch 19/100\u001b[0m\n",
      "Train Loss: 0.62\t|\tTrain Acc: 84.13%\n",
      "Val Loss: 0.42\t|\tVal Acc: 86.79%\n",
      "The current temperature is: 1.360691547857318\n",
      "The current learning rate is: 0.04567701435686405\n",
      "========================================\n",
      "\u001b[1;34mEpoch 20/100\u001b[0m\n",
      "Train Loss: 0.59\t|\tTrain Acc: 84.45%\n",
      "Val Loss: 0.47\t|\tVal Acc: 85.77%\n",
      "The current temperature is: 1.2164149287143908\n",
      "The current learning rate is: 0.04522542485937369\n",
      "========================================\n",
      "\u001b[1;34mEpoch 21/100\u001b[0m\n",
      "Train Loss: 0.59\t|\tTrain Acc: 84.53%\n",
      "Val Loss: 0.41\t|\tVal Acc: 87.75%\n",
      "The current temperature is: 1.1255206586543467\n",
      "The current learning rate is: 0.04475387530939227\n",
      "========================================\n",
      "\u001b[1;34mEpoch 22/100\u001b[0m\n",
      "Train Loss: 0.56\t|\tTrain Acc: 85.42%\n",
      "Val Loss: 0.42\t|\tVal Acc: 87.05%\n",
      "The current temperature is: 1.070291568846434\n",
      "The current learning rate is: 0.04426283106939474\n",
      "========================================\n",
      "\u001b[1;34mEpoch 23/100\u001b[0m\n",
      "Train Loss: 0.56\t|\tTrain Acc: 85.56%\n",
      "Val Loss: 0.50\t|\tVal Acc: 84.06%\n",
      "The current temperature is: 1.0379574471770745\n",
      "The current learning rate is: 0.043752776740761494\n",
      "========================================\n",
      "\u001b[1;34mEpoch 24/100\u001b[0m\n",
      "Train Loss: 0.53\t|\tTrain Acc: 86.84%\n",
      "Val Loss: 0.39\t|\tVal Acc: 88.12%\n",
      "The current temperature is: 1.0197378725320787\n",
      "The current learning rate is: 0.0432242156855353\n",
      "========================================\n",
      "\u001b[1;34mEpoch 25/100\u001b[0m\n",
      "Train Loss: 0.52\t|\tTrain Acc: 86.98%\n",
      "Val Loss: 0.41\t|\tVal Acc: 87.35%\n",
      "The current temperature is: 1.0098689362660394\n",
      "The current learning rate is: 0.042677669529663696\n",
      "========================================\n",
      "\u001b[1;34mEpoch 26/100\u001b[0m\n",
      "Train Loss: 0.53\t|\tTrain Acc: 86.54%\n",
      "Val Loss: 0.40\t|\tVal Acc: 88.17%\n",
      "The current temperature is: 1.0047370894076988\n",
      "The current learning rate is: 0.04211367764821722\n",
      "========================================\n",
      "\u001b[1;34mEpoch 27/100\u001b[0m\n",
      "Train Loss: 0.51\t|\tTrain Acc: 87.37%\n",
      "Val Loss: 0.40\t|\tVal Acc: 88.48%\n",
      "The current temperature is: 1.0021790611275414\n",
      "The current learning rate is: 0.0415327966330913\n",
      "========================================\n",
      "\u001b[1;34mEpoch 29/100\u001b[0m\n",
      "Train Loss: 0.51\t|\tTrain Acc: 87.20%\n",
      "Val Loss: 0.38\t|\tVal Acc: 88.23%\n",
      "The current temperature is: 1.0004026904963697\n",
      "The current learning rate is: 0.040322676341324415\n",
      "========================================\n",
      "\u001b[1;34mEpoch 30/100\u001b[0m\n",
      "Train Loss: 0.48\t|\tTrain Acc: 88.39%\n",
      "Val Loss: 0.40\t|\tVal Acc: 87.83%\n",
      "The current temperature is: 1.0001610761985478\n",
      "The current learning rate is: 0.03969463130731183\n",
      "========================================\n",
      "\u001b[1;34mEpoch 31/100\u001b[0m\n",
      "Train Loss: 0.47\t|\tTrain Acc: 88.58%\n",
      "Val Loss: 0.36\t|\tVal Acc: 88.89%\n",
      "The current temperature is: 1.0000612089554481\n",
      "The current learning rate is: 0.03905208444630327\n",
      "========================================\n",
      "\u001b[1;34mEpoch 32/100\u001b[0m\n",
      "Train Loss: 0.49\t|\tTrain Acc: 87.97%\n",
      "Val Loss: 0.37\t|\tVal Acc: 88.55%\n",
      "The current temperature is: 1.0000220352239613\n",
      "The current learning rate is: 0.038395669874474916\n",
      "========================================\n",
      "\u001b[1;34mEpoch 33/100\u001b[0m\n",
      "Train Loss: 0.50\t|\tTrain Acc: 87.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "\u001b[1;34mEpoch 49/100\u001b[0m\n",
      "Train Loss: 0.44\t|\tTrain Acc: 89.03%\n",
      "Val Loss: 0.35\t|\tVal Acc: 89.63%\n",
      "The current temperature is: 1.0\n",
      "The current learning rate is: 0.025785268976953213\n",
      "========================================\n",
      "\u001b[1;34mEpoch 50/100\u001b[0m\n",
      "Train Loss: 0.48\t|\tTrain Acc: 87.70%\n",
      "Val Loss: 0.35\t|\tVal Acc: 89.93%\n",
      "The current temperature is: 1.0\n",
      "The current learning rate is: 0.025\n",
      "========================================\n",
      "\u001b[1;34mEpoch 51/100\u001b[0m\n",
      "Train Loss: 0.41\t|\tTrain Acc: 89.93%\n",
      "Val Loss: 0.35\t|\tVal Acc: 90.04%\n",
      "The current learning rate is: 0.024214731023046793\n",
      "========================================\n",
      "\u001b[1;34mEpoch 52/100\u001b[0m\n",
      "Train Loss: 0.44\t|\tTrain Acc: 89.37%\n",
      "Val Loss: 0.35\t|\tVal Acc: 89.73%\n",
      "The current learning rate is: 0.023430237011767167\n",
      "========================================\n",
      "\u001b[1;34mEpoch 53/100\u001b[0m\n",
      "Train Loss: 0.44\t|\tTrain Acc: 88.61%\n",
      "Val Loss: 0.34\t|\tVal Acc: 90.11%\n",
      "The current learning rate is: 0.022647292167037147\n",
      "========================================\n",
      "\u001b[1;34mEpoch 54/100\u001b[0m\n",
      "Train Loss: 0.43\t|\tTrain Acc: 89.21%\n",
      "Val Loss: 0.34\t|\tVal Acc: 90.37%\n",
      "The current learning rate is: 0.0218666691608924\n",
      "========================================\n",
      "\u001b[1;34mEpoch 55/100\u001b[0m\n",
      "Train Loss: 0.51\t|\tTrain Acc: 86.89%\n",
      "Val Loss: 0.37\t|\tVal Acc: 89.82%\n",
      "The current learning rate is: 0.021089138373994232\n",
      "========================================\n",
      "\u001b[1;34mEpoch 56/100\u001b[0m\n",
      "Train Loss: 0.40\t|\tTrain Acc: 90.28%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 57/100\u001b[0m\n",
      "Train Loss: 0.47\t|\tTrain Acc: 87.49%\n",
      "Val Loss: 0.37\t|\tVal Acc: 90.06%\n",
      "The current learning rate is: 0.019546418965086437\n",
      "========================================\n",
      "\u001b[1;34mEpoch 58/100\u001b[0m\n",
      "Train Loss: 0.41\t|\tTrain Acc: 90.17%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 73/100\u001b[0m\n",
      "Train Loss: 0.41\t|\tTrain Acc: 89.87%\n",
      "Val Loss: 0.32\t|\tVal Acc: 90.93%\n",
      "The current learning rate is: 0.0084672033669087\n",
      "========================================\n",
      "\u001b[1;34mEpoch 74/100\u001b[0m\n",
      "Train Loss: 0.45\t|\tTrain Acc: 88.19%\n",
      "Val Loss: 0.31\t|\tVal Acc: 90.96%\n",
      "The current learning rate is: 0.00788632235178279\n",
      "========================================\n",
      "\u001b[1;34mEpoch 75/100\u001b[0m\n",
      "Train Loss: 0.36\t|\tTrain Acc: 90.82%\n",
      "Val Loss: 0.31\t|\tVal Acc: 90.94%\n",
      "The current learning rate is: 0.0073223304703363135\n",
      "========================================\n",
      "\u001b[1;34mEpoch 76/100\u001b[0m\n",
      "Train Loss: 0.36\t|\tTrain Acc: 91.35%\n",
      "Val Loss: 0.32\t|\tVal Acc: 91.06%\n",
      "The current learning rate is: 0.006775784314464709\n",
      "========================================\n",
      "\u001b[1;34mEpoch 77/100\u001b[0m\n",
      "Train Loss: 0.40\t|\tTrain Acc: 89.61%\n",
      "Val Loss: 0.32\t|\tVal Acc: 90.80%\n",
      "The current learning rate is: 0.0062472232592385105\n",
      "========================================\n",
      "\u001b[1;34mEpoch 78/100\u001b[0m\n",
      "Train Loss: 0.40\t|\tTrain Acc: 90.25%\n",
      "Val Loss: 0.32\t|\tVal Acc: 90.89%\n",
      "The current learning rate is: 0.005737168930605269\n",
      "========================================\n",
      "\u001b[1;34mEpoch 79/100\u001b[0m\n",
      "Train Loss: 0.41\t|\tTrain Acc: 89.43%\n",
      "Val Loss: 0.32\t|\tVal Acc: 91.01%\n",
      "The current learning rate is: 0.005246124690607737\n",
      "========================================\n",
      "\u001b[1;34mEpoch 80/100\u001b[0m\n",
      "Train Loss: 0.42\t|\tTrain Acc: 89.23%\n",
      "Val Loss: 0.32\t|\tVal Acc: 91.00%\n",
      "The current learning rate is: 0.004774575140626314\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 100\n",
    "# Huáº¥n luyá»‡n mÃ´ hÃ¬nh\n",
    "train_loss, val_loss = [], []\n",
    "train_acc, val_acc = [], []\n",
    "\n",
    "epoch_bar = tqdm(desc = 'Epoch',\n",
    "                 total = num_epochs, position = 1)\n",
    "train_bar = tqdm(desc = 'Training', total = len(train_loader),\n",
    "                 position = 1, leave = True)\n",
    "val_bar = tqdm(desc = 'Validation', total = len(test_loader),\n",
    "               position = 1, leave = True)\n",
    "print(\"ðŸš€ Training MobileNetV3 - Omni Dimensional Dynamic Convolution ðŸš€\")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    epoch_bar.set_description(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "    mb_v3.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    total = 0\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "\n",
    "\n",
    "        if epoch < 50:\n",
    "            temp = get_temperature(i + 1, epoch, len(train_loader),\n",
    "                                   temp_epoch = 50, temp_init = temperature)\n",
    "            mb_v3.net_update_temperature(temp)\n",
    "            # print(f\"The temperature is: {mb_v3.display_temperature()}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        X, y = X.to(args.device), y.to(args.device)\n",
    "\n",
    "        X, y_origin, y_sampled, lam = mixup_data(X, y, args.device,\n",
    "                                                 alpha = 0.2)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = mb_v3(X)\n",
    "        # loss = criterion(output, y)\n",
    "        loss = mixup_criterion(criterion, output, y_origin, y_sampled, lam)\n",
    "\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (i + 1)\n",
    "        total_loss += loss_t\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        temp_lr = adjust_learning_rate(optimizer, epoch, 100,\n",
    "                                       i + 1, len(train_loader),\n",
    "                                       initial_lr = learning_rate)\n",
    "\n",
    "        # Calculating the accuracy\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        n_correct = (lam * predicted.eq(y_origin.data).cpu().sum().float()\n",
    "                    + (1 - lam) * predicted.eq(y_sampled.data).cpu().sum().float())\n",
    "        # n_correct = (predicted == y).sum().item()\n",
    "        acc_t = n_correct / len(predicted) * 100\n",
    "        running_acc += (acc_t - running_acc) / (i + 1)\n",
    "\n",
    "        total_acc += n_correct\n",
    "        total += y.shape[0]\n",
    "\n",
    "        train_bar.set_postfix(loss = running_loss,\n",
    "                              acc = f\"{running_acc:.2f}%\",\n",
    "                              epoch = epoch + 1)\n",
    "        train_bar.update()\n",
    "\n",
    "    current_loss = total_loss / len(train_loader)\n",
    "    current_acc = total_acc / total * 100\n",
    "    train_loss.append(current_loss)\n",
    "    train_acc.append(current_acc)\n",
    "\n",
    "    print(\"========================================\")\n",
    "    print(\"\\033[1;34m\" + f\"Epoch {epoch + 1}/{num_epochs}\" + \"\\033[0m\")\n",
    "    print(f\"Train Loss: {current_loss:.2f}\\t|\\tTrain Acc: {current_acc:.2f}%\")\n",
    "    logging.info(\"========================================\")\n",
    "    logging.info(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    logging.info(f\"Train Loss: {current_loss:.2f}\")\n",
    "    logging.info(f\"Train Acc: {current_acc:.2f}\")\n",
    "    \n",
    "    # Eval trÃªn valid set\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    total = 0\n",
    "    mb_v3.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(test_loader):\n",
    "\n",
    "            X, y = X.to(args.device), y.to(args.device)\n",
    "            # Forward pass\n",
    "            output = mb_v3(X)\n",
    "\n",
    "            # Calculate Loss\n",
    "            loss = criterion(output, y)\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (i + 1)\n",
    "            total_loss += loss_t\n",
    "\n",
    "            # Calculate Accuracies\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            n_correct = (predicted == y).sum().item()\n",
    "            acc_t = n_correct / len(predicted) * 100\n",
    "            running_acc += (acc_t - running_acc) / (i + 1)\n",
    "            total_acc += n_correct\n",
    "\n",
    "            total += y.shape[0]\n",
    "\n",
    "            val_bar.set_postfix(loss = running_loss,\n",
    "                                acc = f\"{running_acc:.2f}%\",\n",
    "                                epoch = epoch + 1)\n",
    "            val_bar.update()\n",
    "\n",
    "    current_loss = total_loss / len(test_loader)\n",
    "    current_acc = total_acc / total * 100\n",
    "\n",
    "    val_loss.append(current_loss)\n",
    "    val_acc.append(current_acc)\n",
    "\n",
    "    print(f\"Val Loss: {current_loss:.2f}\\t|\\tVal Acc: {current_acc:.2f}%\")\n",
    "    logging.info(f\"Val Loss: {current_loss:.2f}\")\n",
    "    logging.info(f\"Val Acc: {current_acc:.2f}\")\n",
    "    \n",
    "    train_bar.n = 0\n",
    "    val_bar.n = 0\n",
    "    epoch_bar.update()\n",
    "\n",
    "    if epoch < 50:\n",
    "        temperature = mb_v3.display_temperature()\n",
    "        print(f\"The current temperature is: {temperature}\")\n",
    "\n",
    "    print(f\"The current learning rate is: {temp_lr}\")\n",
    "\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"Training Completed! ðŸ˜€\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cac0e0-0ada-4fa4-b9ca-6a5565fadcd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
