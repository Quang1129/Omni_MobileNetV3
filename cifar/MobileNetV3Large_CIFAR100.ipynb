{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b13ffa-ef67-49d6-96db-0e706ead5e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd\n",
    "# from omni import *\n",
    "from mixup import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79e881c5-2752-45be-8c9d-847f70e2b93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a MobileNetV3 Model as defined in:\n",
    "Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam. (2019).\n",
    "Searching for MobileNetV3\n",
    "arXiv preprint arXiv:1905.02244.\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "__all__ = ['mobilenetv3_large', 'mobilenetv3_small']\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class h_sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = nn.ReLU6(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "\n",
    "\n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(channel, _make_divisible(channel // reduction, 8)),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(_make_divisible(channel // reduction, 8), channel),\n",
    "                h_sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "def conv_3x3_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        h_swish()\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        h_swish()\n",
    "    )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se, use_hs):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        self.identity = stride == 1 and inp == oup\n",
    "\n",
    "        if inp == hidden_dim:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "    def __init__(self, cfgs, mode, num_classes=1000, width_mult=1.):\n",
    "        super(MobileNetV3, self).__init__()\n",
    "        # setting of inverted residual blocks\n",
    "        self.cfgs = cfgs\n",
    "        assert mode in ['large', 'small']\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(16 * width_mult, 8)\n",
    "        layers = [conv_3x3_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        block = InvertedResidual\n",
    "        for k, t, c, use_se, use_hs, s in self.cfgs:\n",
    "            output_channel = _make_divisible(c * width_mult, 8)\n",
    "            exp_size = _make_divisible(input_channel * t, 8)\n",
    "            layers.append(block(input_channel, exp_size, output_channel, k, s, use_se, use_hs))\n",
    "            input_channel = output_channel\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        # building last several layers\n",
    "        self.conv = conv_1x1_bn(input_channel, exp_size)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        output_channel = {'large': 1280, 'small': 1024}\n",
    "        output_channel = _make_divisible(output_channel[mode] * width_mult, 8) if width_mult > 1.0 else output_channel[mode]\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(exp_size, output_channel),\n",
    "            h_swish(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(output_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def mobilenetv3_large(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV3-Large model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # k, t, c, SE, HS, s\n",
    "        [3,   1,  16, 0, 0, 1],\n",
    "        [3,   4,  24, 0, 0, 2],\n",
    "        [3,   3,  24, 0, 0, 1],\n",
    "        [5,   3,  40, 1, 0, 2],\n",
    "        [5,   3,  40, 1, 0, 1],\n",
    "        [5,   3,  40, 1, 0, 1],\n",
    "        [3,   6,  80, 0, 1, 2],\n",
    "        [3, 2.5,  80, 0, 1, 1],\n",
    "        [3, 2.3,  80, 0, 1, 1],\n",
    "        [3, 2.3,  80, 0, 1, 1],\n",
    "        [3,   6, 112, 1, 1, 1],\n",
    "        [3,   6, 112, 1, 1, 1],\n",
    "        [5,   6, 160, 1, 1, 2],\n",
    "        [5,   6, 160, 1, 1, 1],\n",
    "        [5,   6, 160, 1, 1, 1]\n",
    "    ]\n",
    "    return MobileNetV3(cfgs, mode='large', **kwargs)\n",
    "\n",
    "\n",
    "def mobilenetv3_small(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV3-Small model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # k, t, c, SE, HS, s\n",
    "        [3,    1,  16, 1, 0, 2],\n",
    "        [3,  4.5,  24, 0, 0, 2],\n",
    "        [3, 3.67,  24, 0, 0, 1],\n",
    "        [5,    4,  40, 1, 1, 2],\n",
    "        [5,    6,  40, 1, 1, 1],\n",
    "        [5,    6,  40, 1, 1, 1],\n",
    "        [5,    3,  48, 1, 1, 1],\n",
    "        [5,    3,  48, 1, 1, 1],\n",
    "        [5,    6,  96, 1, 1, 2],\n",
    "        [5,    6,  96, 1, 1, 1],\n",
    "        [5,    6,  96, 1, 1, 1],\n",
    "    ]\n",
    "\n",
    "    return MobileNetV3(cfgs, mode='small', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f59436-0502-480f-8fcf-94a81bf4ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12157d4-0114-4eb5-86f0-30820c145f53",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dd2c471-1968-4798-8452-9646f8302a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be1a163-8d55-4f88-a0ce-d597a7081366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar100/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169001437/169001437 [00:21<00:00, 7926044.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar100/cifar-100-python.tar.gz to ./data/cifar100\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_dir, download = True):\n",
    "\n",
    "  transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "  ])\n",
    "\n",
    "  train_data = datasets.CIFAR100(\n",
    "      root = data_dir, train = True,\n",
    "      download = download, transform = transform\n",
    "  )\n",
    "\n",
    "  test_data = datasets.CIFAR100(\n",
    "      root = data_dir, train = False,\n",
    "      download = download, transform = transform\n",
    "  )\n",
    "\n",
    "  return (train_data, test_data)\n",
    "\n",
    "train_data, test_data = load_data('./data/cifar100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a193e04-e3d7-47dd-b687-135792d2ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size,\n",
    "                          shuffle = True, num_workers = num_workers)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size,\n",
    "                         shuffle = True, num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1ecf7f8-4894-4787-b301-946eb5d8053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acfbf63c-415b-458b-87c5-67a1890f7d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def check_logging_directory(path):\n",
    "  parent_directory = os.path.dirname(path)\n",
    "  if not os.path.exists(parent_directory):\n",
    "    os.makedirs(parent_directory)\n",
    "    print(\"Create new directory\")\n",
    "\n",
    "logging_path = './logging/mixup_mobilenetv3_normallarge_cifar100.log'\n",
    "check_logging_directory(logging_path)\n",
    "\n",
    "logging.basicConfig(filename=logging_path, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a8af43a-71e8-4924-bb25-7dd0faf0d051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters: 4330132\n"
     ]
    }
   ],
   "source": [
    "from gradient_descent_the_ultimate_optimizer import gdtuo\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "mobile_v3 = mobilenetv3_large(num_classes = 100).to(device)\n",
    "optim = gdtuo.Adam(optimizer=gdtuo.SGD(1e-5))\n",
    "mw = gdtuo.ModuleWrapper(mobile_v3, optimizer=optim)\n",
    "mw.initialize()\n",
    "\n",
    "print(f\"The number of parameters: {count_parameters(mobile_v3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6503a504-0939-4499-bfa7-5bcc22e75974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU6'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "FLOPs: 0.233686456 billion\n",
      "Parameters: 4.330132 million\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "\n",
    "input_size = (1, 3, 224, 224)\n",
    "\n",
    "flops, params = profile(mobile_v3, inputs=(torch.randn(*input_size).to(device),))\n",
    "print(f\"FLOPs: {flops / 1e9} billion\")\n",
    "print(f\"Parameters: {params / 1e6} million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d604aa02-fb1c-476c-a590-843558ab6f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d04a7ee3ff477e913b06415af171c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d03c57c46cf460eadf01ed132f1c901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018e5c32f784491da6fc8ab2c10dfd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training MobileNetV3 - Omni Dimensional Dynamic Convolution ðŸš€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343967769/work/torch/csrc/autograd/engine.cpp:1151.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "\u001b[1;34mEpoch 1/100\u001b[0m\n",
      "Train Loss: 4.04\t|\tTrain Acc: 8.65%\n",
      "Val Loss: 3.44\t|\tVal Acc: 16.88%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 2/100\u001b[0m\n",
      "Train Loss: 3.63\t|\tTrain Acc: 16.24%\n",
      "Val Loss: 2.97\t|\tVal Acc: 25.72%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 3/100\u001b[0m\n",
      "Train Loss: 3.33\t|\tTrain Acc: 22.08%\n",
      "Val Loss: 2.62\t|\tVal Acc: 33.48%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 4/100\u001b[0m\n",
      "Train Loss: 3.15\t|\tTrain Acc: 26.26%\n",
      "Val Loss: 2.43\t|\tVal Acc: 38.43%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 5/100\u001b[0m\n",
      "Train Loss: 2.81\t|\tTrain Acc: 33.96%\n",
      "Val Loss: 2.20\t|\tVal Acc: 43.18%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 6/100\u001b[0m\n",
      "Train Loss: 2.64\t|\tTrain Acc: 38.44%\n",
      "Val Loss: 2.06\t|\tVal Acc: 46.14%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 7/100\u001b[0m\n",
      "Train Loss: 2.61\t|\tTrain Acc: 39.66%\n",
      "Val Loss: 1.99\t|\tVal Acc: 47.90%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 8/100\u001b[0m\n",
      "Train Loss: 2.43\t|\tTrain Acc: 44.12%\n",
      "Val Loss: 1.86\t|\tVal Acc: 49.94%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 9/100\u001b[0m\n",
      "Train Loss: 2.39\t|\tTrain Acc: 46.01%\n",
      "Val Loss: 1.75\t|\tVal Acc: 52.75%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 10/100\u001b[0m\n",
      "Train Loss: 2.24\t|\tTrain Acc: 49.67%\n",
      "Val Loss: 1.71\t|\tVal Acc: 54.29%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 11/100\u001b[0m\n",
      "Train Loss: 2.05\t|\tTrain Acc: 54.29%\n",
      "Val Loss: 1.64\t|\tVal Acc: 55.69%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 12/100\u001b[0m\n",
      "Train Loss: 2.09\t|\tTrain Acc: 54.24%\n",
      "Val Loss: 1.66\t|\tVal Acc: 55.70%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 13/100\u001b[0m\n",
      "Train Loss: 2.04\t|\tTrain Acc: 55.45%\n",
      "Val Loss: 1.59\t|\tVal Acc: 57.54%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 14/100\u001b[0m\n",
      "Train Loss: 1.92\t|\tTrain Acc: 58.75%\n",
      "Val Loss: 1.67\t|\tVal Acc: 55.67%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 15/100\u001b[0m\n",
      "Train Loss: 1.86\t|\tTrain Acc: 60.52%\n",
      "Val Loss: 1.57\t|\tVal Acc: 57.94%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 16/100\u001b[0m\n",
      "Train Loss: 1.75\t|\tTrain Acc: 64.17%\n",
      "Val Loss: 1.67\t|\tVal Acc: 56.33%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 17/100\u001b[0m\n",
      "Train Loss: 1.74\t|\tTrain Acc: 64.49%\n",
      "Val Loss: 1.89\t|\tVal Acc: 52.87%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 18/100\u001b[0m\n",
      "Train Loss: 1.78\t|\tTrain Acc: 63.56%\n",
      "Val Loss: 1.54\t|\tVal Acc: 59.16%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 19/100\u001b[0m\n",
      "Train Loss: 1.59\t|\tTrain Acc: 69.22%\n",
      "Val Loss: 1.55\t|\tVal Acc: 59.18%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 20/100\u001b[0m\n",
      "Train Loss: 1.57\t|\tTrain Acc: 69.93%\n",
      "Val Loss: 1.54\t|\tVal Acc: 59.62%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 21/100\u001b[0m\n",
      "Train Loss: 1.50\t|\tTrain Acc: 71.70%\n",
      "Val Loss: 1.60\t|\tVal Acc: 58.82%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 23/100\u001b[0m\n",
      "Train Loss: 1.54\t|\tTrain Acc: 71.08%\n",
      "Val Loss: 1.57\t|\tVal Acc: 60.26%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 24/100\u001b[0m\n",
      "Train Loss: 1.45\t|\tTrain Acc: 73.57%\n",
      "Val Loss: 1.57\t|\tVal Acc: 59.78%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 25/100\u001b[0m\n",
      "Train Loss: 1.39\t|\tTrain Acc: 74.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "\u001b[1;34mEpoch 26/100\u001b[0m\n",
      "Train Loss: 1.42\t|\tTrain Acc: 74.31%\n",
      "Val Loss: 1.59\t|\tVal Acc: 60.03%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 27/100\u001b[0m\n",
      "Train Loss: 1.45\t|\tTrain Acc: 74.15%\n",
      "Val Loss: 1.60\t|\tVal Acc: 60.00%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 28/100\u001b[0m\n",
      "Train Loss: 1.36\t|\tTrain Acc: 75.95%\n",
      "Val Loss: 1.58\t|\tVal Acc: 60.33%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 29/100\u001b[0m\n",
      "Train Loss: 1.46\t|\tTrain Acc: 73.44%\n",
      "Val Loss: 1.63\t|\tVal Acc: 59.31%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 30/100\u001b[0m\n",
      "Train Loss: 1.41\t|\tTrain Acc: 74.65%\n",
      "Val Loss: 1.73\t|\tVal Acc: 57.30%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 31/100\u001b[0m\n",
      "Train Loss: 1.41\t|\tTrain Acc: 74.14%\n",
      "Val Loss: 1.65\t|\tVal Acc: 59.23%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 32/100\u001b[0m\n",
      "Train Loss: 1.45\t|\tTrain Acc: 74.19%\n",
      "Val Loss: 1.61\t|\tVal Acc: 60.14%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 33/100\u001b[0m\n",
      "Train Loss: 1.28\t|\tTrain Acc: 77.50%\n",
      "Val Loss: 1.60\t|\tVal Acc: 60.34%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 34/100\u001b[0m\n",
      "Train Loss: 1.42\t|\tTrain Acc: 74.77%\n",
      "Val Loss: 1.62\t|\tVal Acc: 60.08%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 37/100\u001b[0m\n",
      "Train Loss: 1.28\t|\tTrain Acc: 77.48%\n",
      "Val Loss: 1.60\t|\tVal Acc: 60.10%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 38/100\u001b[0m\n",
      "Train Loss: 1.40\t|\tTrain Acc: 75.33%\n",
      "Val Loss: 1.62\t|\tVal Acc: 60.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "\u001b[1;34mEpoch 46/100\u001b[0m\n",
      "Train Loss: 1.31\t|\tTrain Acc: 76.98%\n",
      "Val Loss: 1.57\t|\tVal Acc: 61.33%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 47/100\u001b[0m\n",
      "Train Loss: 1.35\t|\tTrain Acc: 76.26%\n",
      "Val Loss: 1.67\t|\tVal Acc: 59.86%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 48/100\u001b[0m\n",
      "Train Loss: 1.27\t|\tTrain Acc: 77.88%\n",
      "Val Loss: 1.61\t|\tVal Acc: 60.54%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 49/100\u001b[0m\n",
      "Train Loss: 1.23\t|\tTrain Acc: 78.95%\n",
      "Val Loss: 1.54\t|\tVal Acc: 62.08%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 50/100\u001b[0m\n",
      "Train Loss: 1.21\t|\tTrain Acc: 79.34%\n",
      "Val Loss: 1.65\t|\tVal Acc: 60.26%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 51/100\u001b[0m\n",
      "Train Loss: 1.35\t|\tTrain Acc: 76.32%\n",
      "Val Loss: 1.65\t|\tVal Acc: 60.18%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 52/100\u001b[0m\n",
      "Train Loss: 1.24\t|\tTrain Acc: 78.33%\n",
      "Val Loss: 1.53\t|\tVal Acc: 62.30%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 53/100\u001b[0m\n",
      "Train Loss: 1.21\t|\tTrain Acc: 79.56%\n",
      "Val Loss: 1.58\t|\tVal Acc: 61.49%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 54/100\u001b[0m\n",
      "Train Loss: 1.34\t|\tTrain Acc: 76.63%\n",
      "Val Loss: 1.55\t|\tVal Acc: 61.83%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 56/100\u001b[0m\n",
      "Train Loss: 1.24\t|\tTrain Acc: 78.10%\n",
      "Val Loss: 1.54\t|\tVal Acc: 62.68%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 57/100\u001b[0m\n",
      "Train Loss: 1.25\t|\tTrain Acc: 78.35%\n",
      "Val Loss: 1.54\t|\tVal Acc: 62.82%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 59/100\u001b[0m\n",
      "Train Loss: 1.20\t|\tTrain Acc: 79.23%\n",
      "Val Loss: 1.54\t|\tVal Acc: 62.65%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 60/100\u001b[0m\n",
      "Train Loss: 1.22\t|\tTrain Acc: 78.93%\n",
      "Val Loss: 1.55\t|\tVal Acc: 62.68%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 61/100\u001b[0m\n",
      "Train Loss: 1.24\t|\tTrain Acc: 77.97%\n",
      "Val Loss: 1.58\t|\tVal Acc: 62.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "\u001b[1;34mEpoch 71/100\u001b[0m\n",
      "Train Loss: 1.08\t|\tTrain Acc: 81.58%\n",
      "Val Loss: 1.53\t|\tVal Acc: 64.06%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 72/100\u001b[0m\n",
      "Train Loss: 1.21\t|\tTrain Acc: 78.75%\n",
      "Val Loss: 1.55\t|\tVal Acc: 62.46%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 73/100\u001b[0m\n",
      "Train Loss: 1.25\t|\tTrain Acc: 78.18%\n",
      "Val Loss: 1.57\t|\tVal Acc: 62.03%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 74/100\u001b[0m\n",
      "Train Loss: 1.17\t|\tTrain Acc: 79.29%\n",
      "Val Loss: 1.51\t|\tVal Acc: 62.91%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 75/100\u001b[0m\n",
      "Train Loss: 1.25\t|\tTrain Acc: 78.46%\n",
      "Val Loss: 1.56\t|\tVal Acc: 62.17%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 76/100\u001b[0m\n",
      "Train Loss: 1.24\t|\tTrain Acc: 78.64%\n",
      "Val Loss: 1.53\t|\tVal Acc: 63.68%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 77/100\u001b[0m\n",
      "Train Loss: 1.26\t|\tTrain Acc: 77.67%\n",
      "Val Loss: 1.53\t|\tVal Acc: 62.89%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 78/100\u001b[0m\n",
      "Train Loss: 1.20\t|\tTrain Acc: 79.15%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 80/100\u001b[0m\n",
      "Train Loss: 1.14\t|\tTrain Acc: 80.16%\n",
      "Val Loss: 1.49\t|\tVal Acc: 63.83%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 81/100\u001b[0m\n",
      "Train Loss: 1.16\t|\tTrain Acc: 80.03%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 83/100\u001b[0m\n",
      "Train Loss: 1.26\t|\tTrain Acc: 77.24%\n",
      "Val Loss: 1.48\t|\tVal Acc: 64.58%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 84/100\u001b[0m\n",
      "Train Loss: 1.17\t|\tTrain Acc: 79.91%\n",
      "Val Loss: 1.54\t|\tVal Acc: 63.59%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 85/100\u001b[0m\n",
      "Train Loss: 1.23\t|\tTrain Acc: 78.39%\n",
      "Val Loss: 1.50\t|\tVal Acc: 63.98%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 86/100\u001b[0m\n",
      "Train Loss: 1.14\t|\tTrain Acc: 80.28%\n",
      "Val Loss: 1.49\t|\tVal Acc: 64.59%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 87/100\u001b[0m\n",
      "Train Loss: 1.23\t|\tTrain Acc: 78.13%\n",
      "Val Loss: 1.49\t|\tVal Acc: 64.47%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 88/100\u001b[0m\n",
      "Train Loss: 1.12\t|\tTrain Acc: 80.27%\n",
      "Val Loss: 1.52\t|\tVal Acc: 63.91%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 89/100\u001b[0m\n",
      "Train Loss: 1.22\t|\tTrain Acc: 78.55%\n",
      "Val Loss: 1.51\t|\tVal Acc: 64.29%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 90/100\u001b[0m\n",
      "Train Loss: 1.20\t|\tTrain Acc: 79.22%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 91/100\u001b[0m\n",
      "Train Loss: 1.17\t|\tTrain Acc: 79.56%\n",
      "Val Loss: 1.49\t|\tVal Acc: 64.15%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 92/100\u001b[0m\n",
      "Train Loss: 1.24\t|\tTrain Acc: 78.01%\n",
      "Val Loss: 1.53\t|\tVal Acc: 63.31%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 93/100\u001b[0m\n",
      "Train Loss: 1.19\t|\tTrain Acc: 78.61%\n",
      "Val Loss: 1.53\t|\tVal Acc: 63.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "\u001b[1;34mEpoch 94/100\u001b[0m\n",
      "Train Loss: 1.13\t|\tTrain Acc: 80.41%\n",
      "Val Loss: 1.49\t|\tVal Acc: 64.53%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 95/100\u001b[0m\n",
      "Train Loss: 1.08\t|\tTrain Acc: 81.40%\n",
      "Val Loss: 1.47\t|\tVal Acc: 64.68%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 96/100\u001b[0m\n",
      "Train Loss: 1.11\t|\tTrain Acc: 80.93%\n",
      "Val Loss: 1.48\t|\tVal Acc: 65.00%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 97/100\u001b[0m\n",
      "Train Loss: 1.20\t|\tTrain Acc: 78.54%\n",
      "Val Loss: 1.49\t|\tVal Acc: 64.09%\n"
     ]
    }
   ],
   "source": [
    "# Huáº¥n luyá»‡n mÃ´ hÃ¬nh\n",
    "train_loss, val_loss = [], []\n",
    "train_acc, val_acc = [], []\n",
    "\n",
    "epoch_bar = tqdm(desc = 'Epoch',\n",
    "                 total = num_epochs, position = 1)\n",
    "train_bar = tqdm(desc = 'Training', total = len(train_loader),\n",
    "                 position = 1, leave = True)\n",
    "val_bar = tqdm(desc = 'Validation', total = len(test_loader),\n",
    "               position = 1, leave = True)\n",
    "\n",
    "print(\"ðŸš€ Training MobileNetV3 - Omni Dimensional Dynamic Convolution ðŸš€\")\n",
    "logging.info(\"ðŸš€ Training MobileNetV3 - Omni Dimensional Dynamic Convolution ðŸš€\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    epoch_bar.set_description(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    total = 0\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "\n",
    "        mw.begin()\n",
    "        mw.zero_grad()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        X, y_origin, y_sampled, lam = mixup_data(X, y, device, alpha = 0.4)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = mw.forward(X)\n",
    "        loss = mixup_criterion(criterion, output, y_origin, y_sampled, lam)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward(create_graph = True)\n",
    "        mw.step()\n",
    "    \n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (i + 1)\n",
    "        total_loss += loss_t\n",
    "        \n",
    "        # Calculating the accuracy\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        n_correct = (lam * predicted.eq(y_origin.data).cpu().sum().float()\n",
    "                    + (1 - lam) * predicted.eq(y_sampled.data).cpu().sum().float())\n",
    "\n",
    "        acc_t = n_correct / len(predicted) * 100\n",
    "        running_acc += (acc_t - running_acc) / (i + 1)\n",
    "\n",
    "        total_acc += n_correct\n",
    "        total += y.shape[0]\n",
    "\n",
    "\n",
    "        train_bar.set_postfix(loss = running_loss,\n",
    "                              acc = f\"{running_acc:.2f}%\",\n",
    "                              epoch = epoch + 1)\n",
    "        train_bar.update()\n",
    "\n",
    "    # mw.begin()\n",
    "    current_loss = total_loss / len(train_loader)\n",
    "    current_acc = total_acc / total * 100\n",
    "    train_loss.append(current_loss)\n",
    "    train_acc.append(current_acc)\n",
    "\n",
    "    print(\"========================================\")\n",
    "    print(\"\\033[1;34m\" + f\"Epoch {epoch + 1}/{num_epochs}\" + \"\\033[0m\")\n",
    "    print(f\"Train Loss: {current_loss:.2f}\\t|\\tTrain Acc: {current_acc:.2f}%\")\n",
    "\n",
    "    logging.info(\"========================================\")\n",
    "    logging.info(\"\\033[1;34m\" + f\"Epoch {epoch + 1}/{num_epochs}\" + \"\\033[0m\")\n",
    "    logging.info(f\"Train Loss: {current_loss:.2f}  -   Train Acc: {current_acc:.2f}%\")\n",
    "\n",
    "\n",
    "    # Eval trÃªn valid set\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    total = 0\n",
    "    # mw.end()\n",
    "    # mw.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(test_loader):\n",
    "\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Forward pass\n",
    "            output = mw.forward(X)\n",
    "\n",
    "            # Calculate Loss\n",
    "            loss = criterion(output, y)\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (i + 1)\n",
    "            total_loss += loss_t\n",
    "\n",
    "            # Calculate Accuracies\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            n_correct = (predicted == y).sum().item()\n",
    "            acc_t = n_correct / len(predicted) * 100\n",
    "            running_acc += (acc_t - running_acc) / (i + 1)\n",
    "            total_acc += n_correct\n",
    "\n",
    "            total += y.shape[0]\n",
    "\n",
    "            val_bar.set_postfix(loss = running_loss,\n",
    "                                acc = f\"{running_acc:.2f}%\",\n",
    "                                epoch = epoch + 1)\n",
    "            val_bar.update()\n",
    "\n",
    "    current_loss = total_loss / len(test_loader)\n",
    "    current_acc = total_acc / total * 100\n",
    "\n",
    "    val_loss.append(current_loss)\n",
    "    val_acc.append(current_acc)\n",
    "\n",
    "    print(f\"Val Loss: {current_loss:.2f}\\t|\\tVal Acc: {current_acc:.2f}%\")\n",
    "    logging.info(f\"Val Loss: {current_loss:.2f}  -  Val Acc: {current_acc:.2f}%\")\n",
    "\n",
    "    train_bar.n = 0\n",
    "    val_bar.n = 0\n",
    "    epoch_bar.update()\n",
    "\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"Training Completed! ðŸ˜€\")\n",
    "logging.info(\"========================================\")\n",
    "logging.info(\"Training Completed! ðŸ˜€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2bfe8b-96b2-461c-829e-36586668c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not os.path.exists(\"/plot\"):\n",
    "    os.makedirs(\"/plot\")\n",
    "    print(\"Creating a new directory\")\n",
    "\n",
    "def plot_loss(train_loss, val_loss, loss_fig):\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    plt.plot(range(num_epochs), train_loss)\n",
    "    plt.plot(range(num_epochs), val_loss)\n",
    "\n",
    "    plt.xlabel(\"Num Epochs\")\n",
    "    plt.ylabel(\"Loss Value\")\n",
    "    \n",
    "    plt.savefig(loss_fig)\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(train_acc, val_acc, acc_fig):\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    plt.plot(range(num_epochs), train_acc)\n",
    "    plt.plot(range(num_epochs), val_acc)\n",
    "\n",
    "    plt.xlabel(\"Num Epochs\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    \n",
    "    plt.savefig(acc_fig)\n",
    "    plt.show()\n",
    "\n",
    "loss_path = \"./plot/loss_fig_normal_cifar100(large).png\"\n",
    "acc_path = \"./plot/acc_fig_normal_cifar100(large).png\"\n",
    "\n",
    "plot_loss(train_loss, val_loss, loss_path)\n",
    "plot_accuracy(train_acc, val_acc, acc_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
