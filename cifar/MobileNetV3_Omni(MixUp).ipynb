{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbe99bc0-b98f-4f98-b57a-c10e4beaa4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd\n",
    "from omni import *\n",
    "from mixup import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "511f3d35-94a9-4e55-ab6d-318a4de53397",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a MobileNetV3 Model as defined in:\n",
    "Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam. (2019).\n",
    "Searching for MobileNetV3\n",
    "arXiv preprint arXiv:1905.02244.\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "__all__ = ['mobilenetv3_large', 'mobilenetv3_small']\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class h_sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = nn.ReLU6(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "\n",
    "\n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(channel, _make_divisible(channel // reduction, 8)),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(_make_divisible(channel // reduction, 8), channel),\n",
    "                h_sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "def conv_3x3_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, kernel_size = 3, stride = stride,\n",
    "                            padding = 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        h_swish()\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, kernel_size = 1, stride = 1,\n",
    "                            padding = 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        h_swish()\n",
    "    )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se, use_hs, kernel_num = 4):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        self.identity = stride == 1 and inp == oup\n",
    "\n",
    "        if inp == hidden_dim:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                ODConvBN(hidden_dim, hidden_dim, kernel_size, stride, groups=hidden_dim, kernel_num = kernel_num),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                # pw-linear\n",
    "                ODConv2d(hidden_dim, oup, 1, 1),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                ODConvBN(inp, hidden_dim, kernel_size = 1, stride = 1),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # dw\n",
    "                ODConvBN(hidden_dim, hidden_dim, kernel_size, stride, groups=hidden_dim, kernel_num = kernel_num),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # pw-linear\n",
    "                ODConv2d(hidden_dim, oup, 1, 1),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "    def __init__(self, cfgs, mode, num_classes=1000, width_mult=1.):\n",
    "        super(MobileNetV3, self).__init__()\n",
    "        # setting of inverted residual blocks\n",
    "        self.cfgs = cfgs\n",
    "        assert mode in ['large', 'small']\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(16 * width_mult, 8)\n",
    "        layers = [conv_3x3_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        block = InvertedResidual\n",
    "        for k, t, c, use_se, use_hs, s in self.cfgs:\n",
    "            output_channel = _make_divisible(c * width_mult, 8)\n",
    "            exp_size = _make_divisible(input_channel * t, 8)\n",
    "            layers.append(block(input_channel, exp_size, output_channel, k, s, use_se, use_hs, kernel_num = 1))\n",
    "            input_channel = output_channel\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "        # building last several layers\n",
    "        self.conv = conv_1x1_bn(input_channel, exp_size)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        output_channel = {'large': 1280, 'small': 1024}\n",
    "\n",
    "        output_channel = _make_divisible(output_channel[mode] * width_mult, 8) if width_mult > 1.0 else output_channel[mode]\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(exp_size, output_channel),\n",
    "            h_swish(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(output_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def net_update_temperature(self, temperature):\n",
    "        for modules in self.modules():\n",
    "            if hasattr(modules, \"update_temperature\"):\n",
    "                modules.update_temperature(temperature)\n",
    "\n",
    "    def display_temperature(self):\n",
    "        for modules in self.modules():\n",
    "            if hasattr(modules, \"get_temperature\"):\n",
    "                return modules.get_temperature()\n",
    "\n",
    "def mobilenetv3_large(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV3-Large model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # k, t,   c,  SE, HS, s\n",
    "        [3,   1,  16, 0, 0, 1],\n",
    "        [3,   4,  24, 0, 0, 2],\n",
    "        [3,   3,  24, 0, 0, 1],\n",
    "        [5,   3,  40, 1, 0, 2],\n",
    "        [5,   3,  40, 1, 0, 1],\n",
    "        [5,   3,  40, 1, 0, 1],\n",
    "        [3,   6,  80, 0, 1, 2],\n",
    "        [3, 2.5,  80, 0, 1, 1],\n",
    "        [3, 2.3,  80, 0, 1, 1],\n",
    "        [3, 2.3,  80, 0, 1, 1],\n",
    "        [3,   6, 112, 1, 1, 1],\n",
    "        [3,   6, 112, 1, 1, 1],\n",
    "        [5,   6, 160, 1, 1, 2],\n",
    "        [5,   6, 160, 1, 1, 1],\n",
    "        [5,   6, 160, 1, 1, 1]\n",
    "    ]\n",
    "    return MobileNetV3(cfgs, mode='large', **kwargs)\n",
    "\n",
    "\n",
    "def mobilenetv3_small(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV3-Small model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # k,   t,  c, SE, HS, s\n",
    "        [3,    1,  16, 1, 0, 2],\n",
    "        [3,  4.5,  24, 0, 0, 2],\n",
    "        [3, 3.67,  24, 0, 0, 1],\n",
    "        [5,    4,  40, 1, 1, 2],\n",
    "        [5,    6,  40, 1, 1, 1],\n",
    "        [5,    6,  40, 1, 1, 1],\n",
    "        [5,    3,  48, 1, 1, 1],\n",
    "        [5,    3,  48, 1, 1, 1],\n",
    "        [5,    6,  96, 1, 1, 2],\n",
    "        [5,    6,  96, 1, 1, 1],\n",
    "        [5,    6,  96, 1, 1, 1],\n",
    "    ]\n",
    "\n",
    "    return MobileNetV3(cfgs, mode='small', **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88ade2bc-b3d5-4ad3-bf8e-96f3472d5508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def69fa4-e356-43d1-af2e-e5889993a42c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c2ae798-ceb6-4610-ba64-304aba3e1cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7269b1e-25b6-4c82-a33f-b1f83dadff59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170498071/170498071 [01:01<00:00, 2788615.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar10/cifar-10-python.tar.gz to ./data/cifar10\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_dir, download = True):\n",
    "\n",
    "  transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "  ])\n",
    "\n",
    "  train_data = datasets.CIFAR10(\n",
    "      root = data_dir, train = True,\n",
    "      download = download, transform = transform\n",
    "  )\n",
    "\n",
    "  test_data = datasets.CIFAR10(\n",
    "      root = data_dir, train = False,\n",
    "      download = download, transform = transform\n",
    "  )\n",
    "\n",
    "  return (train_data, test_data)\n",
    "\n",
    "train_data, test_data = load_data('./data/cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6025fb9-496d-4820-ba81-dfe7cd7975f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size,\n",
    "                          shuffle = True, num_workers = num_workers)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size,\n",
    "                         shuffle = True, num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea2c6b43-6a2e-4230-8b6e-8575fd80788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0982adb8-9ddd-47aa-8c2c-238bdcf0544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def check_logging_directory(path):\n",
    "  parent_directory = os.path.dirname(path)\n",
    "  if not os.path.exists(parent_directory):\n",
    "    os.makedirs(parent_directory)\n",
    "    print(\"Create new directory\")\n",
    "\n",
    "logging_path = './logging/mixup_mobilenetv3_omni_cifar10.log'\n",
    "check_logging_directory(logging_path)\n",
    "\n",
    "logging.basicConfig(filename=logging_path, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa95a1d0-fe3c-4078-85d5-06439b7f793c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2396571"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "mb_v3 =  mobilenetv3_small(num_classes = 10).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(mb_v3.parameters(), lr=learning_rate,weight_decay=1e-5)\n",
    "\n",
    "\n",
    "count_parameters(mb_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2821a0c-ad36-47ab-8cf2-8b168951ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temperature(iteration, epoch, iter_per_epoch, temp_epoch=10, temp_init=30.0):\n",
    "    total_temp_iter = iter_per_epoch * temp_epoch\n",
    "    current_iter = iteration + epoch * iter_per_epoch\n",
    "    # print(current_iter)\n",
    "    temperature = 1.0 + max(0, (temp_init - 1.0) * ((total_temp_iter - current_iter) / total_temp_iter))\n",
    "    return temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded28beb-1775-4dd7-96fb-af835863e41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d663c0d7ce96488eb06547685e4b685d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c688b446c9d4b1887e7b2d3d4735188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5fa6e3cba347d99a24f9bde3e61468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training MobileNetV3 - Omni Dimensional Dynamic Convolution ðŸš€\n",
      "========================================\n",
      "\u001b[1;34mEpoch 1/100\u001b[0m\n",
      "Train Loss: 1.72\t|\tTrain Acc: 39.22%\n",
      "Val Loss: 1.30\t|\tVal Acc: 55.86%\n",
      "The current temperature is: 29.419999999999998\n",
      "========================================\n",
      "\u001b[1;34mEpoch 2/100\u001b[0m\n",
      "Train Loss: 1.41\t|\tTrain Acc: 54.37%\n",
      "Val Loss: 1.07\t|\tVal Acc: 63.94%\n",
      "The current temperature is: 28.84\n",
      "========================================\n",
      "\u001b[1;34mEpoch 3/100\u001b[0m\n",
      "Train Loss: 1.24\t|\tTrain Acc: 61.17%\n",
      "Val Loss: 0.83\t|\tVal Acc: 72.00%\n",
      "The current temperature is: 28.259999999999998\n",
      "========================================\n",
      "\u001b[1;34mEpoch 4/100\u001b[0m\n",
      "Train Loss: 1.21\t|\tTrain Acc: 63.11%\n",
      "Val Loss: 0.78\t|\tVal Acc: 76.53%\n",
      "The current temperature is: 27.68\n",
      "========================================\n",
      "\u001b[1;34mEpoch 5/100\u001b[0m\n",
      "Train Loss: 1.13\t|\tTrain Acc: 66.12%\n",
      "Val Loss: 0.76\t|\tVal Acc: 77.72%\n",
      "The current temperature is: 27.1\n",
      "========================================\n",
      "\u001b[1;34mEpoch 6/100\u001b[0m\n",
      "Train Loss: 1.04\t|\tTrain Acc: 69.74%\n",
      "Val Loss: 0.71\t|\tVal Acc: 78.10%\n",
      "The current temperature is: 26.52\n",
      "========================================\n",
      "\u001b[1;34mEpoch 7/100\u001b[0m\n",
      "Train Loss: 0.99\t|\tTrain Acc: 71.76%\n",
      "Val Loss: 0.68\t|\tVal Acc: 78.71%\n",
      "The current temperature is: 25.94\n",
      "========================================\n",
      "\u001b[1;34mEpoch 8/100\u001b[0m\n",
      "Train Loss: 0.99\t|\tTrain Acc: 71.83%\n",
      "Val Loss: 0.61\t|\tVal Acc: 82.41%\n",
      "The current temperature is: 25.36\n",
      "========================================\n",
      "\u001b[1;34mEpoch 9/100\u001b[0m\n",
      "Train Loss: 1.01\t|\tTrain Acc: 70.70%\n",
      "Val Loss: 0.63\t|\tVal Acc: 82.32%\n",
      "The current temperature is: 24.779999999999998\n",
      "========================================\n",
      "\u001b[1;34mEpoch 10/100\u001b[0m\n",
      "Train Loss: 0.91\t|\tTrain Acc: 74.28%\n",
      "Val Loss: 0.60\t|\tVal Acc: 82.30%\n",
      "The current temperature is: 24.200000000000003\n",
      "========================================\n",
      "\u001b[1;34mEpoch 11/100\u001b[0m\n",
      "Train Loss: 0.95\t|\tTrain Acc: 73.03%\n",
      "Val Loss: 0.62\t|\tVal Acc: 82.98%\n",
      "The current temperature is: 23.62\n",
      "========================================\n",
      "\u001b[1;34mEpoch 12/100\u001b[0m\n",
      "Train Loss: 0.94\t|\tTrain Acc: 72.94%\n",
      "Val Loss: 0.54\t|\tVal Acc: 84.37%\n",
      "The current temperature is: 23.04\n",
      "========================================\n",
      "\u001b[1;34mEpoch 13/100\u001b[0m\n",
      "Train Loss: 0.88\t|\tTrain Acc: 74.87%\n",
      "Val Loss: 0.55\t|\tVal Acc: 84.14%\n",
      "The current temperature is: 22.46\n",
      "========================================\n",
      "\u001b[1;34mEpoch 14/100\u001b[0m\n",
      "Train Loss: 0.78\t|\tTrain Acc: 78.54%\n",
      "Val Loss: 0.55\t|\tVal Acc: 83.93%\n",
      "The current temperature is: 21.88\n",
      "========================================\n",
      "\u001b[1;34mEpoch 15/100\u001b[0m\n",
      "Train Loss: 0.81\t|\tTrain Acc: 77.81%\n",
      "Val Loss: 0.58\t|\tVal Acc: 83.77%\n",
      "The current temperature is: 21.299999999999997\n",
      "========================================\n",
      "\u001b[1;34mEpoch 16/100\u001b[0m\n",
      "Train Loss: 0.81\t|\tTrain Acc: 77.45%\n",
      "Val Loss: 0.59\t|\tVal Acc: 83.77%\n",
      "The current temperature is: 20.720000000000002\n",
      "========================================\n",
      "\u001b[1;34mEpoch 17/100\u001b[0m\n",
      "Train Loss: 0.81\t|\tTrain Acc: 77.35%\n",
      "Val Loss: 0.48\t|\tVal Acc: 84.91%\n",
      "The current temperature is: 20.14\n",
      "========================================\n",
      "\u001b[1;34mEpoch 18/100\u001b[0m\n",
      "Train Loss: 0.77\t|\tTrain Acc: 78.87%\n",
      "Val Loss: 0.49\t|\tVal Acc: 85.23%\n",
      "The current temperature is: 19.56\n",
      "========================================\n",
      "\u001b[1;34mEpoch 19/100\u001b[0m\n",
      "Train Loss: 0.87\t|\tTrain Acc: 76.14%\n",
      "Val Loss: 0.47\t|\tVal Acc: 86.14%\n",
      "The current temperature is: 18.98\n",
      "========================================\n",
      "\u001b[1;34mEpoch 20/100\u001b[0m\n",
      "Train Loss: 0.74\t|\tTrain Acc: 79.59%\n",
      "Val Loss: 0.53\t|\tVal Acc: 85.23%\n",
      "The current temperature is: 18.4\n",
      "========================================\n",
      "\u001b[1;34mEpoch 21/100\u001b[0m\n",
      "Train Loss: 0.79\t|\tTrain Acc: 77.97%\n",
      "Val Loss: 0.48\t|\tVal Acc: 85.79%\n",
      "The current temperature is: 17.82\n",
      "========================================\n",
      "\u001b[1;34mEpoch 22/100\u001b[0m\n",
      "Train Loss: 0.78\t|\tTrain Acc: 79.01%\n",
      "Val Loss: 0.51\t|\tVal Acc: 85.61%\n",
      "The current temperature is: 17.240000000000002\n",
      "========================================\n",
      "\u001b[1;34mEpoch 23/100\u001b[0m\n",
      "Train Loss: 0.71\t|\tTrain Acc: 80.94%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 25/100\u001b[0m\n",
      "Train Loss: 0.76\t|\tTrain Acc: 79.63%\n",
      "Val Loss: 0.47\t|\tVal Acc: 86.42%\n",
      "The current temperature is: 15.5\n",
      "========================================\n",
      "\u001b[1;34mEpoch 26/100\u001b[0m\n",
      "Train Loss: 0.75\t|\tTrain Acc: 79.82%\n",
      "Val Loss: 0.52\t|\tVal Acc: 85.97%\n",
      "The current temperature is: 14.92\n",
      "========================================\n",
      "\u001b[1;34mEpoch 27/100\u001b[0m\n",
      "Train Loss: 0.75\t|\tTrain Acc: 79.79%\n",
      "Val Loss: 0.49\t|\tVal Acc: 85.90%\n",
      "The current temperature is: 14.34\n",
      "========================================\n",
      "\u001b[1;34mEpoch 28/100\u001b[0m\n",
      "Train Loss: 0.82\t|\tTrain Acc: 77.55%\n",
      "Val Loss: 0.54\t|\tVal Acc: 85.00%\n",
      "The current temperature is: 13.76\n",
      "========================================\n",
      "\u001b[1;34mEpoch 29/100\u001b[0m\n",
      "Train Loss: 0.77\t|\tTrain Acc: 79.38%\n",
      "Val Loss: 0.57\t|\tVal Acc: 85.20%\n",
      "The current temperature is: 13.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "\u001b[1;34mEpoch 38/100\u001b[0m\n",
      "Train Loss: 0.64\t|\tTrain Acc: 82.72%\n",
      "Val Loss: 0.47\t|\tVal Acc: 86.40%\n",
      "The current temperature is: 7.96\n",
      "========================================\n",
      "\u001b[1;34mEpoch 39/100\u001b[0m\n",
      "Train Loss: 0.71\t|\tTrain Acc: 80.78%\n",
      "Val Loss: 0.46\t|\tVal Acc: 86.70%\n",
      "The current temperature is: 7.38\n",
      "========================================\n",
      "\u001b[1;34mEpoch 40/100\u001b[0m\n",
      "Train Loss: 0.73\t|\tTrain Acc: 79.91%\n",
      "Val Loss: 0.47\t|\tVal Acc: 87.07%\n",
      "The current temperature is: 6.800000000000001\n",
      "========================================\n",
      "\u001b[1;34mEpoch 41/100\u001b[0m\n",
      "Train Loss: 0.69\t|\tTrain Acc: 81.47%\n",
      "Val Loss: 0.47\t|\tVal Acc: 87.62%\n",
      "The current temperature is: 6.22\n",
      "========================================\n",
      "\u001b[1;34mEpoch 42/100\u001b[0m\n",
      "Train Loss: 0.74\t|\tTrain Acc: 79.51%\n",
      "Val Loss: 0.47\t|\tVal Acc: 87.03%\n",
      "The current temperature is: 5.64\n",
      "========================================\n",
      "\u001b[1;34mEpoch 43/100\u001b[0m\n",
      "Train Loss: 0.70\t|\tTrain Acc: 80.88%\n",
      "Val Loss: 0.46\t|\tVal Acc: 86.97%\n",
      "The current temperature is: 5.0600000000000005\n",
      "========================================\n",
      "\u001b[1;34mEpoch 44/100\u001b[0m\n",
      "Train Loss: 0.68\t|\tTrain Acc: 81.72%\n",
      "Val Loss: 0.46\t|\tVal Acc: 86.92%\n",
      "The current temperature is: 4.48\n",
      "========================================\n",
      "\u001b[1;34mEpoch 45/100\u001b[0m\n",
      "Train Loss: 0.70\t|\tTrain Acc: 80.92%\n",
      "Val Loss: 0.44\t|\tVal Acc: 86.98%\n",
      "The current temperature is: 3.9000000000000004\n",
      "========================================\n",
      "\u001b[1;34mEpoch 46/100\u001b[0m\n",
      "Train Loss: 0.69\t|\tTrain Acc: 81.39%\n",
      "Val Loss: 0.47\t|\tVal Acc: 86.18%\n",
      "The current temperature is: 3.32\n",
      "========================================\n",
      "\u001b[1;34mEpoch 47/100\u001b[0m\n",
      "Train Loss: 0.72\t|\tTrain Acc: 80.59%\n",
      "Val Loss: 0.46\t|\tVal Acc: 86.75%\n",
      "The current temperature is: 2.74\n",
      "========================================\n",
      "\u001b[1;34mEpoch 48/100\u001b[0m\n",
      "Train Loss: 0.73\t|\tTrain Acc: 79.89%\n",
      "Val Loss: 0.53\t|\tVal Acc: 85.80%\n",
      "The current temperature is: 2.16\n",
      "========================================\n",
      "\u001b[1;34mEpoch 49/100\u001b[0m\n",
      "Train Loss: 0.70\t|\tTrain Acc: 81.15%\n",
      "Val Loss: 0.53\t|\tVal Acc: 85.19%\n",
      "The current temperature is: 1.58\n",
      "========================================\n",
      "\u001b[1;34mEpoch 50/100\u001b[0m\n",
      "Train Loss: 0.69\t|\tTrain Acc: 81.01%\n",
      "Val Loss: 0.57\t|\tVal Acc: 83.42%\n",
      "The current temperature is: 1.0\n",
      "========================================\n",
      "\u001b[1;34mEpoch 51/100\u001b[0m\n",
      "Train Loss: 0.67\t|\tTrain Acc: 81.89%\n",
      "Val Loss: 0.46\t|\tVal Acc: 87.21%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 52/100\u001b[0m\n",
      "Train Loss: 0.73\t|\tTrain Acc: 79.77%\n",
      "Val Loss: 0.47\t|\tVal Acc: 86.85%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 53/100\u001b[0m\n",
      "Train Loss: 0.62\t|\tTrain Acc: 83.79%\n",
      "Val Loss: 0.47\t|\tVal Acc: 86.64%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 54/100\u001b[0m\n",
      "Train Loss: 0.72\t|\tTrain Acc: 80.86%\n",
      "Val Loss: 0.46\t|\tVal Acc: 86.94%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 55/100\u001b[0m\n",
      "Train Loss: 0.68\t|\tTrain Acc: 81.82%\n",
      "Val Loss: 0.46\t|\tVal Acc: 87.64%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 56/100\u001b[0m\n",
      "Train Loss: 0.65\t|\tTrain Acc: 82.62%\n",
      "Val Loss: 0.43\t|\tVal Acc: 87.85%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 57/100\u001b[0m\n",
      "Train Loss: 0.64\t|\tTrain Acc: 82.35%\n",
      "Val Loss: 0.43\t|\tVal Acc: 88.35%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 58/100\u001b[0m\n",
      "Train Loss: 0.64\t|\tTrain Acc: 82.72%\n",
      "Val Loss: 0.42\t|\tVal Acc: 88.51%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 59/100\u001b[0m\n",
      "Train Loss: 0.64\t|\tTrain Acc: 82.79%\n",
      "Val Loss: 0.43\t|\tVal Acc: 88.15%\n"
     ]
    }
   ],
   "source": [
    "# Huáº¥n luyá»‡n mÃ´ hÃ¬nh\n",
    "train_loss, val_loss = [], []\n",
    "train_acc, val_acc = [], []\n",
    "\n",
    "epoch_bar = tqdm(desc = 'Epoch',\n",
    "                 total = num_epochs, position = 1)\n",
    "train_bar = tqdm(desc = 'Training', total = len(train_loader),\n",
    "                 position = 1, leave = True)\n",
    "val_bar = tqdm(desc = 'Validation', total = len(test_loader),\n",
    "               position = 1, leave = True)\n",
    "\n",
    "print(\"ðŸš€ Training MobileNetV3 - Omni Dimensional Dynamic Convolution ðŸš€\")\n",
    "logging.info(\"ðŸš€ Training MobileNetV3 - Omni Dimensional Dynamic Convolution ðŸš€\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    epoch_bar.set_description(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "    mb_v3.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    total = 0\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "\n",
    "        if epoch < 50:\n",
    "            temp = get_temperature(i + 1, epoch, len(train_loader), temp_epoch = 50, temp_init = 60.0)\n",
    "            mb_v3.net_update_temperature(temp)\n",
    "            # print(f\"The temperature is: {temp}\")\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        X, y_origin, y_sampled, lam = mixup_data(X, y, device, alpha = 0.4)\n",
    " \n",
    "        # Forward pass\n",
    "        output = mb_v3(X)\n",
    "        loss = criterion(output, y)\n",
    "        loss = mixup_criterion(criterion, output, y_origin, y_sampled, lam)\n",
    "        \n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (i + 1)\n",
    "        total_loss += loss_t\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculating the accuracy\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        n_correct = (lam * predicted.eq(y_origin.data).cpu().sum().float()\n",
    "                    + (1 - lam) * predicted.eq(y_sampled.data).cpu().sum().float())\n",
    "\n",
    "        acc_t = n_correct / len(predicted) * 100\n",
    "        running_acc += (acc_t - running_acc) / (i + 1)\n",
    "\n",
    "        total_acc += n_correct\n",
    "        total += y.shape[0]\n",
    "\n",
    "\n",
    "        train_bar.set_postfix(loss = running_loss,\n",
    "                              acc = f\"{running_acc:.2f}%\",\n",
    "                              epoch = epoch + 1)\n",
    "        train_bar.update()\n",
    "\n",
    "    current_loss = total_loss / len(train_loader)\n",
    "    current_acc = total_acc / total * 100\n",
    "    train_loss.append(current_loss)\n",
    "    train_acc.append(current_acc)\n",
    "\n",
    "    print(\"========================================\")\n",
    "    print(\"\\033[1;34m\" + f\"Epoch {epoch + 1}/{num_epochs}\" + \"\\033[0m\")\n",
    "    print(f\"Train Loss: {current_loss:.2f}\\t|\\tTrain Acc: {current_acc:.2f}%\")\n",
    "\n",
    "    logging.info(\"========================================\")\n",
    "    logging.info(\"\\033[1;34m\" + f\"Epoch {epoch + 1}/{num_epochs}\" + \"\\033[0m\")\n",
    "    logging.info(f\"Train Loss: {current_loss:.2f}  -   Train Acc: {current_acc:.2f}%\")\n",
    "\n",
    "\n",
    "    # Eval trÃªn valid set\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    total = 0\n",
    "    mb_v3.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(test_loader):\n",
    "\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Forward pass\n",
    "            output = mb_v3(X)\n",
    "\n",
    "            # Calculate Loss\n",
    "            loss = criterion(output, y)\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (i + 1)\n",
    "            total_loss += loss_t\n",
    "\n",
    "            # Calculate Accuracies\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            n_correct = (predicted == y).sum().item()\n",
    "            acc_t = n_correct / len(predicted) * 100\n",
    "            running_acc += (acc_t - running_acc) / (i + 1)\n",
    "            total_acc += n_correct\n",
    "\n",
    "            total += y.shape[0]\n",
    "\n",
    "            val_bar.set_postfix(loss = running_loss,\n",
    "                                acc = f\"{running_acc:.2f}%\",\n",
    "                                epoch = epoch + 1)\n",
    "            val_bar.update()\n",
    "\n",
    "    current_loss = total_loss / len(test_loader)\n",
    "    current_acc = total_acc / total * 100\n",
    "\n",
    "    val_loss.append(current_loss)\n",
    "    val_acc.append(current_acc)\n",
    "\n",
    "    print(f\"Val Loss: {current_loss:.2f}\\t|\\tVal Acc: {current_acc:.2f}%\")\n",
    "    logging.info(f\"Val Loss: {current_loss:.2f}  -  Val Acc: {current_acc:.2f}%\")\n",
    "\n",
    "    train_bar.n = 0\n",
    "    val_bar.n = 0\n",
    "    epoch_bar.update()\n",
    "\n",
    "    if epoch < 50:\n",
    "        temperature = mb_v3.display_temperature()\n",
    "        print(f\"The current temperature is: {temperature}\")\n",
    "        \n",
    "    # if epoch < 10: \n",
    "    #     temperature = mb_v3.display_temperature()\n",
    "    #     print(f\"The current temperature is: {temperature}\")\n",
    "    #     mb_v3.net_update_temperature(temperature - 3)\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"Training Completed! ðŸ˜€\")\n",
    "logging.info(\"========================================\")\n",
    "logging.info(\"Training Completed! ðŸ˜€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9d75f8-6d50-40a6-9a67-3cf70b721017",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c803bf8a-7ef8-446a-8db6-bec6f290f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not os.path.exists(\"/plot\"):\n",
    "    os.makedirs(\"/plot\")\n",
    "    print(\"Creating a new directory\")\n",
    "\n",
    "def plot_loss(train_loss, val_loss, loss_fig):\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    plt.plot(range(num_epochs), train_loss)\n",
    "    plt.plot(range(num_epochs), val_loss)\n",
    "\n",
    "    plt.xlabel(\"Num Epochs\")\n",
    "    plt.ylabel(\"Loss Value\")\n",
    "    \n",
    "    plt.savefig(loss_fig)\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(train_acc, val_acc, acc_fig):\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    plt.plot(range(num_epochs), train_acc)\n",
    "    plt.plot(range(num_epochs), val_acc)\n",
    "\n",
    "    plt.xlabel(\"Num Epochs\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    \n",
    "    plt.savefig(acc_fig)\n",
    "    plt.show()\n",
    "\n",
    "loss_path = \"./plot/loss_fig_omni_cifar10.png\"f\n",
    "acc_path = \"./plot/acc_fig_omni_cifar10.png\"\n",
    "\n",
    "plot_loss(train_loss, val_loss, loss_path)\n",
    "plot_accuracy(train_acc, val_acc, acc_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3c723-5c6c-4560-acf0-219fafad9609",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lowest_loss = np.argmin(train_loss)\n",
    "train_highest_acc = np.argmax(train_acc)\n",
    "print(f\"The index of lowest train loss: {train_lowest_loss + 1}\")\n",
    "print(f\"Train Loss: {train_loss[train_lowest_loss]}\")\n",
    "print(f\"The index of highest train accuracy: {train_highest_acc + 1}\")\n",
    "print(f\"Train Loss: {train_acc[train_highest_acc]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd103456-c051-4180-9ab2-81cdc72fd052",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_lowest_loss = np.argmin(val_loss)\n",
    "val_highest_acc = np.argmax(val_acc)\n",
    "print(f\"The index of lowest train loss: {val_lowest_loss + 1}\")\n",
    "print(f\"Train Loss: {val_loss[val_lowest_loss]}\")\n",
    "print(f\"The index of highest train accuracy: {val_highest_acc + 1}\")\n",
    "print(f\"Train Loss: {val_acc[val_highest_acc]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee336fc6-6d23-4a81-924d-d9a9dfc3b683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
