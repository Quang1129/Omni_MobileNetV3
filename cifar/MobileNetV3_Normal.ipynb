{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c29986-d4fa-4f83-a2ea-699cb21cb6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd\n",
    "# from omni import *\n",
    "from mixup import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f641c992-ae7d-4de2-be12-5b092cc5a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a MobileNetV3 Model as defined in:\n",
    "Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam. (2019).\n",
    "Searching for MobileNetV3\n",
    "arXiv preprint arXiv:1905.02244.\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "__all__ = ['mobilenetv3_large', 'mobilenetv3_small']\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class h_sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = nn.ReLU6(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "\n",
    "\n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(channel, _make_divisible(channel // reduction, 8)),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(_make_divisible(channel // reduction, 8), channel),\n",
    "                h_sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "def conv_3x3_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        h_swish()\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        h_swish()\n",
    "    )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se, use_hs):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        self.identity = stride == 1 and inp == oup\n",
    "\n",
    "        if inp == hidden_dim:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "    def __init__(self, cfgs, mode, num_classes=1000, width_mult=1.):\n",
    "        super(MobileNetV3, self).__init__()\n",
    "        # setting of inverted residual blocks\n",
    "        self.cfgs = cfgs\n",
    "        assert mode in ['large', 'small']\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(16 * width_mult, 8)\n",
    "        layers = [conv_3x3_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        block = InvertedResidual\n",
    "        for k, t, c, use_se, use_hs, s in self.cfgs:\n",
    "            output_channel = _make_divisible(c * width_mult, 8)\n",
    "            exp_size = _make_divisible(input_channel * t, 8)\n",
    "            layers.append(block(input_channel, exp_size, output_channel, k, s, use_se, use_hs))\n",
    "            input_channel = output_channel\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        # building last several layers\n",
    "        self.conv = conv_1x1_bn(input_channel, exp_size)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        output_channel = {'large': 1280, 'small': 1024}\n",
    "        output_channel = _make_divisible(output_channel[mode] * width_mult, 8) if width_mult > 1.0 else output_channel[mode]\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(exp_size, output_channel),\n",
    "            h_swish(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(output_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def mobilenetv3_large(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV3-Large model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # k, t, c, SE, HS, s\n",
    "        [3,   1,  16, 0, 0, 1],\n",
    "        [3,   4,  24, 0, 0, 2],\n",
    "        [3,   3,  24, 0, 0, 1],\n",
    "        [5,   3,  40, 1, 0, 2],\n",
    "        [5,   3,  40, 1, 0, 1],\n",
    "        [5,   3,  40, 1, 0, 1],\n",
    "        [3,   6,  80, 0, 1, 2],\n",
    "        [3, 2.5,  80, 0, 1, 1],\n",
    "        [3, 2.3,  80, 0, 1, 1],\n",
    "        [3, 2.3,  80, 0, 1, 1],\n",
    "        [3,   6, 112, 1, 1, 1],\n",
    "        [3,   6, 112, 1, 1, 1],\n",
    "        [5,   6, 160, 1, 1, 2],\n",
    "        [5,   6, 160, 1, 1, 1],\n",
    "        [5,   6, 160, 1, 1, 1]\n",
    "    ]\n",
    "    return MobileNetV3(cfgs, mode='large', **kwargs)\n",
    "\n",
    "\n",
    "def mobilenetv3_small(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV3-Small model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # k, t, c, SE, HS, s\n",
    "        [3,    1,  16, 1, 0, 2],\n",
    "        [3,  4.5,  24, 0, 0, 2],\n",
    "        [3, 3.67,  24, 0, 0, 1],\n",
    "        [5,    4,  40, 1, 1, 2],\n",
    "        [5,    6,  40, 1, 1, 1],\n",
    "        [5,    6,  40, 1, 1, 1],\n",
    "        [5,    3,  48, 1, 1, 1],\n",
    "        [5,    3,  48, 1, 1, 1],\n",
    "        [5,    6,  96, 1, 1, 2],\n",
    "        [5,    6,  96, 1, 1, 1],\n",
    "        [5,    6,  96, 1, 1, 1],\n",
    "    ]\n",
    "\n",
    "    return MobileNetV3(cfgs, mode='small', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c8676ad-5f87-4480-9c74-ce9c20222f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c819b8-d80e-4b5c-9da9-e4380b51bb7b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13113f4e-390e-49c6-a71f-022f90d9fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffd1c0d1-0edc-48f9-9ac4-67c356fecd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_dir, download = True):\n",
    "\n",
    "  transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "  ])\n",
    "\n",
    "  train_data = datasets.CIFAR10(\n",
    "      root = data_dir, train = True,\n",
    "      download = download, transform = transform\n",
    "  )\n",
    "\n",
    "  test_data = datasets.CIFAR10(\n",
    "      root = data_dir, train = False,\n",
    "      download = download, transform = transform\n",
    "  )\n",
    "\n",
    "  return (train_data, test_data)\n",
    "\n",
    "train_data, test_data = load_data('./data/cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00602ee7-c524-472d-bfb8-f37acd96c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size,\n",
    "                          shuffle = True, num_workers = num_workers)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size,\n",
    "                         shuffle = True, num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c0c4016-0cbd-4fb1-8dc2-8ae28e41107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2e6e9a5-1ffc-44a5-8db8-a3324c720c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def check_logging_directory(path):\n",
    "  parent_directory = os.path.dirname(path)\n",
    "  if not os.path.exists(parent_directory):\n",
    "    os.makedirs(parent_directory)\n",
    "    print(\"Create new directory\")\n",
    "\n",
    "logging_path = './logging/mixup_mobilenetv3_normal.log'\n",
    "check_logging_directory(logging_path)\n",
    "\n",
    "logging.basicConfig(filename=logging_path, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8847fb0d-d282-44e0-86c4-340b286b868e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1528106"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "mb_v3 =  mobilenetv3_small(num_classes = 10).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(mb_v3.parameters(), lr=learning_rate,weight_decay=1e-5)\n",
    "\n",
    "\n",
    "count_parameters(mb_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5e0b5-f697-450c-a6d3-c0b2ee35b1bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59da7267217d4d1a9faa1f6e4d1dce70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59140a10f9cf432fb52f635bb3306f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6de73645c94c33a7f2b923d20598b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training MobileNetV3 - Omni Dimensional Dynamic Convolution ðŸš€\n",
      "========================================\n",
      "\u001b[1;34mEpoch 1/100\u001b[0m\n",
      "Train Loss: 1.76\t|\tTrain Acc: 37.00%\n",
      "Val Loss: 1.32\t|\tVal Acc: 52.58%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 2/100\u001b[0m\n",
      "Train Loss: 1.41\t|\tTrain Acc: 54.44%\n",
      "Val Loss: 0.98\t|\tVal Acc: 66.40%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 3/100\u001b[0m\n",
      "Train Loss: 1.24\t|\tTrain Acc: 61.64%\n",
      "Val Loss: 0.81\t|\tVal Acc: 73.61%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 4/100\u001b[0m\n",
      "Train Loss: 1.16\t|\tTrain Acc: 65.15%\n",
      "Val Loss: 0.77\t|\tVal Acc: 77.04%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 5/100\u001b[0m\n",
      "Train Loss: 1.02\t|\tTrain Acc: 69.83%\n",
      "Val Loss: 0.70\t|\tVal Acc: 78.91%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 6/100\u001b[0m\n",
      "Train Loss: 1.05\t|\tTrain Acc: 69.47%\n",
      "Val Loss: 0.61\t|\tVal Acc: 80.80%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 7/100\u001b[0m\n",
      "Train Loss: 1.00\t|\tTrain Acc: 70.87%\n",
      "Val Loss: 0.64\t|\tVal Acc: 80.27%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 8/100\u001b[0m\n",
      "Train Loss: 0.94\t|\tTrain Acc: 73.39%\n",
      "Val Loss: 0.63\t|\tVal Acc: 79.83%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 9/100\u001b[0m\n",
      "Train Loss: 0.95\t|\tTrain Acc: 73.17%\n",
      "Val Loss: 0.54\t|\tVal Acc: 83.33%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 10/100\u001b[0m\n",
      "Train Loss: 0.98\t|\tTrain Acc: 71.73%\n",
      "Val Loss: 0.57\t|\tVal Acc: 82.92%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 11/100\u001b[0m\n",
      "Train Loss: 0.83\t|\tTrain Acc: 76.77%\n",
      "Val Loss: 0.54\t|\tVal Acc: 83.51%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 12/100\u001b[0m\n",
      "Train Loss: 0.83\t|\tTrain Acc: 77.64%\n",
      "Val Loss: 0.53\t|\tVal Acc: 84.13%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 13/100\u001b[0m\n",
      "Train Loss: 0.85\t|\tTrain Acc: 76.27%\n",
      "Val Loss: 0.55\t|\tVal Acc: 83.97%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 14/100\u001b[0m\n",
      "Train Loss: 0.83\t|\tTrain Acc: 76.86%\n",
      "Val Loss: 0.55\t|\tVal Acc: 84.09%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 15/100\u001b[0m\n",
      "Train Loss: 0.82\t|\tTrain Acc: 77.57%\n",
      "Val Loss: 0.48\t|\tVal Acc: 85.03%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 16/100\u001b[0m\n",
      "Train Loss: 0.76\t|\tTrain Acc: 79.60%\n",
      "Val Loss: 0.49\t|\tVal Acc: 85.29%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 17/100\u001b[0m\n",
      "Train Loss: 0.82\t|\tTrain Acc: 77.17%\n",
      "Val Loss: 0.49\t|\tVal Acc: 85.46%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 18/100\u001b[0m\n",
      "Train Loss: 0.74\t|\tTrain Acc: 80.74%\n",
      "Val Loss: 0.48\t|\tVal Acc: 85.35%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 19/100\u001b[0m\n",
      "Train Loss: 0.79\t|\tTrain Acc: 78.64%\n",
      "Val Loss: 0.59\t|\tVal Acc: 82.91%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 20/100\u001b[0m\n",
      "Train Loss: 0.77\t|\tTrain Acc: 79.79%\n",
      "Val Loss: 0.47\t|\tVal Acc: 85.79%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 21/100\u001b[0m\n",
      "Train Loss: 0.82\t|\tTrain Acc: 77.45%\n",
      "Val Loss: 0.49\t|\tVal Acc: 85.75%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 22/100\u001b[0m\n",
      "Train Loss: 0.73\t|\tTrain Acc: 80.70%\n",
      "Val Loss: 0.52\t|\tVal Acc: 84.39%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 23/100\u001b[0m\n",
      "Train Loss: 0.72\t|\tTrain Acc: 80.74%\n",
      "Val Loss: 0.49\t|\tVal Acc: 86.02%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 24/100\u001b[0m\n",
      "Train Loss: 0.73\t|\tTrain Acc: 80.78%\n",
      "Val Loss: 0.51\t|\tVal Acc: 85.55%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 25/100\u001b[0m\n",
      "Train Loss: 0.72\t|\tTrain Acc: 81.07%\n",
      "Val Loss: 0.55\t|\tVal Acc: 85.29%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 26/100\u001b[0m\n",
      "Train Loss: 0.79\t|\tTrain Acc: 78.84%\n",
      "Val Loss: 0.51\t|\tVal Acc: 85.62%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 27/100\u001b[0m\n",
      "Train Loss: 0.72\t|\tTrain Acc: 81.24%\n",
      "Val Loss: 0.48\t|\tVal Acc: 86.01%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 28/100\u001b[0m\n",
      "Train Loss: 0.76\t|\tTrain Acc: 79.93%\n",
      "Val Loss: 0.56\t|\tVal Acc: 85.16%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 29/100\u001b[0m\n",
      "Train Loss: 0.64\t|\tTrain Acc: 83.87%\n",
      "Val Loss: 0.53\t|\tVal Acc: 85.21%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 30/100\u001b[0m\n",
      "Train Loss: 0.68\t|\tTrain Acc: 82.47%\n",
      "Val Loss: 0.51\t|\tVal Acc: 86.08%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 31/100\u001b[0m\n",
      "Train Loss: 0.73\t|\tTrain Acc: 80.79%\n",
      "Val Loss: 0.53\t|\tVal Acc: 85.89%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 32/100\u001b[0m\n",
      "Train Loss: 0.66\t|\tTrain Acc: 82.76%\n",
      "Val Loss: 0.51\t|\tVal Acc: 85.29%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 33/100\u001b[0m\n",
      "Train Loss: 0.68\t|\tTrain Acc: 82.11%\n",
      "Val Loss: 0.47\t|\tVal Acc: 86.73%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 34/100\u001b[0m\n",
      "Train Loss: 0.71\t|\tTrain Acc: 81.84%\n",
      "Val Loss: 0.47\t|\tVal Acc: 86.43%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 35/100\u001b[0m\n",
      "Train Loss: 0.70\t|\tTrain Acc: 81.36%\n",
      "Val Loss: 0.46\t|\tVal Acc: 86.71%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 36/100\u001b[0m\n",
      "Train Loss: 0.61\t|\tTrain Acc: 84.31%\n",
      "Val Loss: 0.47\t|\tVal Acc: 86.73%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 37/100\u001b[0m\n",
      "Train Loss: 0.68\t|\tTrain Acc: 81.89%\n",
      "Val Loss: 0.48\t|\tVal Acc: 86.50%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 38/100\u001b[0m\n",
      "Train Loss: 0.66\t|\tTrain Acc: 82.92%\n",
      "Val Loss: 0.50\t|\tVal Acc: 86.05%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 39/100\u001b[0m\n",
      "Train Loss: 0.74\t|\tTrain Acc: 80.04%\n",
      "Val Loss: 0.49\t|\tVal Acc: 86.22%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 40/100\u001b[0m\n",
      "Train Loss: 0.66\t|\tTrain Acc: 82.99%\n",
      "Val Loss: 0.50\t|\tVal Acc: 85.74%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 41/100\u001b[0m\n",
      "Train Loss: 0.66\t|\tTrain Acc: 82.40%\n",
      "Val Loss: 0.48\t|\tVal Acc: 86.76%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 42/100\u001b[0m\n",
      "Train Loss: 0.67\t|\tTrain Acc: 81.76%\n",
      "Val Loss: 0.48\t|\tVal Acc: 87.57%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 43/100\u001b[0m\n",
      "Train Loss: 0.73\t|\tTrain Acc: 80.13%\n",
      "Val Loss: 0.45\t|\tVal Acc: 87.20%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 44/100\u001b[0m\n",
      "Train Loss: 0.67\t|\tTrain Acc: 82.73%\n",
      "Val Loss: 0.47\t|\tVal Acc: 86.73%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 45/100\u001b[0m\n",
      "Train Loss: 0.74\t|\tTrain Acc: 80.10%\n",
      "Val Loss: 0.48\t|\tVal Acc: 86.48%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 46/100\u001b[0m\n",
      "Train Loss: 0.64\t|\tTrain Acc: 83.25%\n",
      "Val Loss: 0.47\t|\tVal Acc: 86.70%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 47/100\u001b[0m\n",
      "Train Loss: 0.62\t|\tTrain Acc: 84.11%\n",
      "Val Loss: 0.53\t|\tVal Acc: 85.89%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 48/100\u001b[0m\n",
      "Train Loss: 0.69\t|\tTrain Acc: 81.84%\n",
      "Val Loss: 0.48\t|\tVal Acc: 86.24%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 49/100\u001b[0m\n",
      "Train Loss: 0.72\t|\tTrain Acc: 80.34%\n",
      "Val Loss: 0.48\t|\tVal Acc: 86.64%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 50/100\u001b[0m\n",
      "Train Loss: 0.61\t|\tTrain Acc: 84.69%\n",
      "Val Loss: 0.47\t|\tVal Acc: 86.70%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 51/100\u001b[0m\n",
      "Train Loss: 0.65\t|\tTrain Acc: 82.12%\n",
      "Val Loss: 0.48\t|\tVal Acc: 86.94%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 52/100\u001b[0m\n",
      "Train Loss: 0.65\t|\tTrain Acc: 82.91%\n",
      "Val Loss: 0.47\t|\tVal Acc: 87.03%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 53/100\u001b[0m\n",
      "Train Loss: 0.64\t|\tTrain Acc: 83.02%\n",
      "Val Loss: 0.47\t|\tVal Acc: 87.01%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 54/100\u001b[0m\n",
      "Train Loss: 0.68\t|\tTrain Acc: 81.53%\n",
      "Val Loss: 0.44\t|\tVal Acc: 87.49%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 55/100\u001b[0m\n",
      "Train Loss: 0.67\t|\tTrain Acc: 82.27%\n",
      "Val Loss: 0.51\t|\tVal Acc: 85.97%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 56/100\u001b[0m\n",
      "Train Loss: 0.66\t|\tTrain Acc: 82.57%\n",
      "Val Loss: 0.46\t|\tVal Acc: 87.19%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 57/100\u001b[0m\n",
      "Train Loss: 0.70\t|\tTrain Acc: 81.61%\n",
      "Val Loss: 0.48\t|\tVal Acc: 87.04%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 58/100\u001b[0m\n",
      "Train Loss: 0.64\t|\tTrain Acc: 83.03%\n",
      "Val Loss: 0.46\t|\tVal Acc: 86.91%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 59/100\u001b[0m\n",
      "Train Loss: 0.70\t|\tTrain Acc: 80.84%\n",
      "Val Loss: 0.43\t|\tVal Acc: 87.74%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 60/100\u001b[0m\n",
      "Train Loss: 0.70\t|\tTrain Acc: 80.92%\n",
      "Val Loss: 0.46\t|\tVal Acc: 87.47%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 61/100\u001b[0m\n",
      "Train Loss: 0.71\t|\tTrain Acc: 81.24%\n",
      "Val Loss: 0.46\t|\tVal Acc: 87.72%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 62/100\u001b[0m\n",
      "Train Loss: 0.69\t|\tTrain Acc: 81.53%\n",
      "Val Loss: 0.44\t|\tVal Acc: 88.00%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 63/100\u001b[0m\n",
      "Train Loss: 0.62\t|\tTrain Acc: 84.19%\n",
      "Val Loss: 0.47\t|\tVal Acc: 87.13%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 67/100\u001b[0m\n",
      "Train Loss: 0.64\t|\tTrain Acc: 82.66%\n",
      "Val Loss: 0.44\t|\tVal Acc: 87.66%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 68/100\u001b[0m\n",
      "Train Loss: 0.66\t|\tTrain Acc: 82.34%\n",
      "Val Loss: 0.43\t|\tVal Acc: 87.76%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 69/100\u001b[0m\n",
      "Train Loss: 0.69\t|\tTrain Acc: 80.96%\n",
      "Val Loss: 0.44\t|\tVal Acc: 87.73%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 70/100\u001b[0m\n",
      "Train Loss: 0.67\t|\tTrain Acc: 81.86%\n",
      "Val Loss: 0.47\t|\tVal Acc: 87.49%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 71/100\u001b[0m\n",
      "Train Loss: 0.70\t|\tTrain Acc: 81.15%\n",
      "Val Loss: 0.43\t|\tVal Acc: 88.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "\u001b[1;34mEpoch 88/100\u001b[0m\n",
      "Train Loss: 0.58\t|\tTrain Acc: 84.91%\n",
      "Val Loss: 0.43\t|\tVal Acc: 88.16%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 89/100\u001b[0m\n",
      "Train Loss: 0.61\t|\tTrain Acc: 83.17%\n",
      "Val Loss: 0.44\t|\tVal Acc: 87.83%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 90/100\u001b[0m\n",
      "Train Loss: 0.62\t|\tTrain Acc: 84.27%\n",
      "Val Loss: 0.41\t|\tVal Acc: 88.38%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 91/100\u001b[0m\n",
      "Train Loss: 0.61\t|\tTrain Acc: 84.40%\n",
      "Val Loss: 0.44\t|\tVal Acc: 88.28%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 92/100\u001b[0m\n",
      "Train Loss: 0.67\t|\tTrain Acc: 81.92%\n",
      "Val Loss: 0.45\t|\tVal Acc: 88.06%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 93/100\u001b[0m\n",
      "Train Loss: 0.61\t|\tTrain Acc: 83.95%\n",
      "Val Loss: 0.45\t|\tVal Acc: 88.00%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 94/100\u001b[0m\n",
      "Train Loss: 0.64\t|\tTrain Acc: 82.57%\n",
      "Val Loss: 0.47\t|\tVal Acc: 87.62%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 95/100\u001b[0m\n",
      "Train Loss: 0.64\t|\tTrain Acc: 82.81%\n",
      "Val Loss: 0.40\t|\tVal Acc: 88.86%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 96/100\u001b[0m\n",
      "Train Loss: 0.68\t|\tTrain Acc: 81.47%\n",
      "Val Loss: 0.44\t|\tVal Acc: 88.33%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 97/100\u001b[0m\n",
      "Train Loss: 0.63\t|\tTrain Acc: 83.39%\n",
      "Val Loss: 0.45\t|\tVal Acc: 88.00%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 98/100\u001b[0m\n",
      "Train Loss: 0.62\t|\tTrain Acc: 83.02%\n",
      "Val Loss: 0.47\t|\tVal Acc: 88.26%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 99/100\u001b[0m\n",
      "Train Loss: 0.63\t|\tTrain Acc: 82.65%\n",
      "Val Loss: 0.47\t|\tVal Acc: 87.18%\n"
     ]
    }
   ],
   "source": [
    "# Huáº¥n luyá»‡n mÃ´ hÃ¬nh\n",
    "train_loss, val_loss = [], []\n",
    "train_acc, val_acc = [], []\n",
    "\n",
    "epoch_bar = tqdm(desc = 'Epoch',\n",
    "                 total = num_epochs, position = 1)\n",
    "train_bar = tqdm(desc = 'Training', total = len(train_loader),\n",
    "                 position = 1, leave = True)\n",
    "val_bar = tqdm(desc = 'Validation', total = len(test_loader),\n",
    "               position = 1, leave = True)\n",
    "\n",
    "print(\"ðŸš€ Training MobileNetV3 - Omni Dimensional Dynamic Convolution ðŸš€\")\n",
    "logging.info(\"ðŸš€ Training MobileNetV3 - Omni Dimensional Dynamic Convolution ðŸš€\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    epoch_bar.set_description(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "    mb_v3.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    total = 0\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        X, y_origin, y_sampled, lam = mixup_data(X, y, device, alpha = 0.4)\n",
    " \n",
    "        # Forward pass\n",
    "        output = mb_v3(X)\n",
    "        loss = mixup_criterion(criterion, output, y_origin, y_sampled, lam)\n",
    "        \n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (i + 1)\n",
    "        total_loss += loss_t\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculating the accuracy\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        n_correct = (lam * predicted.eq(y_origin.data).cpu().sum().float()\n",
    "                    + (1 - lam) * predicted.eq(y_sampled.data).cpu().sum().float())\n",
    "\n",
    "        acc_t = n_correct / len(predicted) * 100\n",
    "        running_acc += (acc_t - running_acc) / (i + 1)\n",
    "\n",
    "        total_acc += n_correct\n",
    "        total += y.shape[0]\n",
    "\n",
    "\n",
    "        train_bar.set_postfix(loss = running_loss,\n",
    "                              acc = f\"{running_acc:.2f}%\",\n",
    "                              epoch = epoch + 1)\n",
    "        train_bar.update()\n",
    "\n",
    "    current_loss = total_loss / len(train_loader)\n",
    "    current_acc = total_acc / total * 100\n",
    "    train_loss.append(current_loss)\n",
    "    train_acc.append(current_acc)\n",
    "\n",
    "    print(\"========================================\")\n",
    "    print(\"\\033[1;34m\" + f\"Epoch {epoch + 1}/{num_epochs}\" + \"\\033[0m\")\n",
    "    print(f\"Train Loss: {current_loss:.2f}\\t|\\tTrain Acc: {current_acc:.2f}%\")\n",
    "\n",
    "    logging.info(\"========================================\")\n",
    "    logging.info(\"\\033[1;34m\" + f\"Epoch {epoch + 1}/{num_epochs}\" + \"\\033[0m\")\n",
    "    logging.info(f\"Train Loss: {current_loss:.2f}  -   Train Acc: {current_acc:.2f}%\")\n",
    "\n",
    "\n",
    "    # Eval trÃªn valid set\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    total = 0\n",
    "    mb_v3.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(test_loader):\n",
    "\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Forward pass\n",
    "            output = mb_v3(X)\n",
    "\n",
    "            # Calculate Loss\n",
    "            loss = criterion(output, y)\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (i + 1)\n",
    "            total_loss += loss_t\n",
    "\n",
    "            # Calculate Accuracies\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            n_correct = (predicted == y).sum().item()\n",
    "            acc_t = n_correct / len(predicted) * 100\n",
    "            running_acc += (acc_t - running_acc) / (i + 1)\n",
    "            total_acc += n_correct\n",
    "\n",
    "            total += y.shape[0]\n",
    "\n",
    "            val_bar.set_postfix(loss = running_loss,\n",
    "                                acc = f\"{running_acc:.2f}%\",\n",
    "                                epoch = epoch + 1)\n",
    "            val_bar.update()\n",
    "\n",
    "    current_loss = total_loss / len(test_loader)\n",
    "    current_acc = total_acc / total * 100\n",
    "\n",
    "    val_loss.append(current_loss)\n",
    "    val_acc.append(current_acc)\n",
    "\n",
    "    print(f\"Val Loss: {current_loss:.2f}\\t|\\tVal Acc: {current_acc:.2f}%\")\n",
    "    logging.info(f\"Val Loss: {current_loss:.2f}  -  Val Acc: {current_acc:.2f}%\")\n",
    "\n",
    "    train_bar.n = 0\n",
    "    val_bar.n = 0\n",
    "    epoch_bar.update()\n",
    "\n",
    "    # if epoch < 10: \n",
    "    #     temperature = mb_v3.display_temperature()\n",
    "    #     print(f\"The current temperature is: {temperature}\")\n",
    "    #     mb_v3.net_update_temperature(temperature - 3)\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"Training Completed! ðŸ˜€\")\n",
    "logging.info(\"========================================\")\n",
    "logging.info(\"Training Completed! ðŸ˜€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d7fc1e-39a4-41b2-a919-317afaceb822",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not os.path.exists(\"/plot\"):\n",
    "    os.makedirs(\"/plot\")\n",
    "    print(\"Creating a new directory\")\n",
    "\n",
    "def plot_loss(train_loss, val_loss, loss_fig):\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    plt.plot(range(num_epochs), train_loss)\n",
    "    plt.plot(range(num_epochs), val_loss)\n",
    "\n",
    "    plt.xlabel(\"Num Epochs\")\n",
    "    plt.ylabel(\"Loss Value\")\n",
    "    \n",
    "    plt.savefig(loss_fig)\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(train_acc, val_acc, acc_fig):\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    plt.plot(range(num_epochs), train_acc)\n",
    "    plt.plot(range(num_epochs), val_acc)\n",
    "\n",
    "    plt.xlabel(\"Num Epochs\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    \n",
    "    plt.savefig(acc_fig)\n",
    "    plt.show()\n",
    "\n",
    "loss_path = \"./plot/loss_fig_normal.png\"\n",
    "acc_path = \"./plot/acc_fig_normal.png\"\n",
    "\n",
    "plot_loss(train_loss, val_loss, loss_path)\n",
    "plot_accuracy(train_acc, val_acc, acc_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2663c893-669b-411b-a591-09a53c25d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lowest_loss = np.argmin(train_loss)\n",
    "train_highest_acc = np.argmax(train_acc)\n",
    "print(f\"The index of lowest train loss: {train_lowest_loss + 1}\")\n",
    "print(f\"Train Loss: {train_loss[train_lowest_loss]}\")\n",
    "print(f\"The index of highest train accuracy: {train_highest_acc + 1}\")\n",
    "print(f\"Train Loss: {train_acc[train_highest_acc]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2de350-73dd-488d-bf51-f4f1f3a606e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_lowest_loss = np.argmin(val_loss)\n",
    "val_highest_acc = np.argmax(val_acc)\n",
    "print(f\"The index of lowest train loss: {val_lowest_loss + 1}\")\n",
    "print(f\"Train Loss: {val_loss[val_lowest_loss]}\")\n",
    "print(f\"The index of highest train accuracy: {val_highest_acc + 1}\")\n",
    "print(f\"Train Loss: {val_acc[val_highest_acc]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617301c0-c294-4706-b50c-ad14b950868e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
